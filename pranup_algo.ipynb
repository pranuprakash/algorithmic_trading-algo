{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranuprakash/anaconda3/lib/python3.11/site-packages/pyfolio/pos.py:26: UserWarning: Module \"zipline.assets\" not found; mutltipliers will not be applied to position notionals.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import backtrader as bt\n",
    "import quantstats as qs\n",
    "import pyfolio as pf\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from fredapi import Fred\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start='2007-01-01'\n",
    "end='2019-10-04'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>CALL</th>\n",
       "      <th>PUT</th>\n",
       "      <th>TOTAL</th>\n",
       "      <th>P/C Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2007-01-03</td>\n",
       "      <td>205724</td>\n",
       "      <td>309416</td>\n",
       "      <td>515140</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2007-01-04</td>\n",
       "      <td>270783</td>\n",
       "      <td>293745</td>\n",
       "      <td>564528</td>\n",
       "      <td>1.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2007-01-05</td>\n",
       "      <td>290570</td>\n",
       "      <td>352167</td>\n",
       "      <td>642737</td>\n",
       "      <td>1.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2007-01-08</td>\n",
       "      <td>232352</td>\n",
       "      <td>273485</td>\n",
       "      <td>505837</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2007-01-09</td>\n",
       "      <td>237668</td>\n",
       "      <td>276434</td>\n",
       "      <td>514102</td>\n",
       "      <td>1.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>582589</td>\n",
       "      <td>987408</td>\n",
       "      <td>1569997</td>\n",
       "      <td>1.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>617097</td>\n",
       "      <td>1050008</td>\n",
       "      <td>1667105</td>\n",
       "      <td>1.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3250</th>\n",
       "      <td>2019-10-02</td>\n",
       "      <td>1376258</td>\n",
       "      <td>1782956</td>\n",
       "      <td>3159214</td>\n",
       "      <td>1.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3251</th>\n",
       "      <td>2019-10-03</td>\n",
       "      <td>769129</td>\n",
       "      <td>1260434</td>\n",
       "      <td>2029563</td>\n",
       "      <td>1.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>2019-10-04</td>\n",
       "      <td>783827</td>\n",
       "      <td>1180047</td>\n",
       "      <td>1963874</td>\n",
       "      <td>1.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3212 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           DATE     CALL      PUT    TOTAL  P/C Ratio\n",
       "41   2007-01-03   205724   309416   515140       1.50\n",
       "42   2007-01-04   270783   293745   564528       1.08\n",
       "43   2007-01-05   290570   352167   642737       1.21\n",
       "44   2007-01-08   232352   273485   505837       1.18\n",
       "45   2007-01-09   237668   276434   514102       1.16\n",
       "...         ...      ...      ...      ...        ...\n",
       "3248 2019-09-30   582589   987408  1569997       1.69\n",
       "3249 2019-10-01   617097  1050008  1667105       1.70\n",
       "3250 2019-10-02  1376258  1782956  3159214       1.30\n",
       "3251 2019-10-03   769129  1260434  2029563       1.64\n",
       "3252 2019-10-04   783827  1180047  1963874       1.51\n",
       "\n",
       "[3212 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexpc = pd.read_csv('indexpc.csv', skiprows=2)\n",
    "#indexpc.columns = ['Date', 'Call', 'Put', 'Total', 'PC_Ratio']\n",
    "\n",
    "# Converting the 'Date' column to datetime format\n",
    "indexpc['DATE'] = pd.to_datetime(indexpc['DATE'], errors='coerce')\n",
    "\n",
    "# Filtering the data\n",
    "indexpc = indexpc[(indexpc['DATE'] >= '2007-01-01') & (indexpc['DATE'] <= '2019-10-04')]\n",
    "\n",
    "# Check the data types\n",
    "indexpc # P/C Ratio = Puts / Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgs2_data = pd.read_csv('DGS2.csv')\n",
    "\n",
    "# Convert the 'DATE' column to datetime and the 'DGS2' column to numeric\n",
    "dgs2_data['DATE'] = pd.to_datetime(dgs2_data['DATE'])\n",
    "dgs2_data['DGS2'] = pd.to_numeric(dgs2_data['DGS2'], errors='coerce')\n",
    "\n",
    "# Calculate the daily percentage change of DGS2 values\n",
    "dgs2_data['DGS2_pct_change'] = dgs2_data['DGS2'].pct_change()\n",
    "\n",
    "# Filter the data for the specified date range (2000-01-01 to 2021-11-12)\n",
    "dgs2_filtered = dgs2_data[(dgs2_data['DATE'] >= start) & (dgs2_data['DATE'] <= end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stock:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.label = []\n",
    "    \n",
    "    #preprocess\n",
    "    #feature creation\n",
    "    def factor(self):\n",
    "        del self.data[\"Close\"]\n",
    "        self.data = self.data.fillna(method = \"bfill\")\n",
    "        \n",
    "        #return\n",
    "        #>0,+1;<=0,-1\n",
    "        self.data['label']=self.data.rolling(2).apply(lambda x:x.iloc[1]>x.iloc[0])['Adj Close']\n",
    "        self.data['label']=self.data.label.shift(-1)\n",
    "\n",
    "        # Volume Change (daily percentage change)\n",
    "        self.data['volume_pct_change'] = self.data['Volume'].pct_change()\n",
    "\n",
    "        # Volume Moving Averages\n",
    "        self.data['vol_ma_5'] = self.data['Volume'].rolling(window=5).mean()\n",
    "        #self.data['vol_ma_10'] = self.data['Volume'].rolling(window=10).mean()\n",
    "        self.data['vol_ma_20'] = self.data['Volume'].rolling(window=20).mean()  \n",
    "        self.data['vol_ma_50'] = self.data['Volume'].rolling(window=50).mean()  \n",
    "        self.data['vol_ma_200'] = self.data['Volume'].rolling(window=200).mean()  \n",
    "\n",
    "        #moving average\n",
    "        self.data[\"ma_5\"] = self.data[\"Adj Close\"].rolling(window = 5).mean()\n",
    "        self.data[\"ma_10\"] = self.data[\"Adj Close\"].rolling(window = 10).mean()\n",
    "        self.data[\"ma_20\"] = self.data[\"Adj Close\"].rolling(window = 20).mean()\n",
    "        self.data[\"ma_50\"] = self.data[\"Adj Close\"].rolling(window = 50).mean()\n",
    "        self.data[\"ma_200\"] = self.data[\"Adj Close\"].rolling(window = 200).mean()\n",
    "\n",
    "        \n",
    "        self.merge_pc_ratio(indexpc)\n",
    "        self.add_volume_oscillator()\n",
    "        self.add_relative_volume()\n",
    "        self.add_volume_relative_to_ma()\n",
    "        self.add_price_volume_trend()\n",
    "        self.add_vix_feature()\n",
    "        self.add_spy_vix_ratio_feature()\n",
    "        self.add_rsi_feature()\n",
    "        self.add_bollinger_bands_feature()\n",
    "        self.add_cci_feature()\n",
    "        \n",
    "        \n",
    "        self.add_10yr_bond_yield_change_feature()\n",
    "        self.add_yield_spread_feature(dgs2_filtered)\n",
    "        self.add_atr_feature()\n",
    "\n",
    "        \n",
    "        #self.add_volume_spikes()\n",
    "        self.add_stochastic_oscillator()\n",
    "        self.add_spy_iwm_ratio_feature()\n",
    "        self.add_spy_qqq_ratio_feature()\n",
    "        self.add_spy_dia_ratio_feature()\n",
    "        self.add_dxy_feature()\n",
    "        self.add_obv_feature()\n",
    "        #self.create_combined_conditions()\n",
    "\n",
    "\n",
    "        self.add_momentum_feature_5d(window=5)\n",
    "        \n",
    "        #self.add_momentum_feature_10d(window=10)\n",
    "        #self.add_momentum_feature_20d(window=20)\n",
    "        #self.add_momentum_feature_50d(window=50)\n",
    "        #self.add_momentum_feature_200d(window=200)\n",
    "\n",
    "\n",
    "        #self.add_volatility_feature_short(window=21)\n",
    "        self.add_volatility_feature_long(window=252)\n",
    "\n",
    "        self.add_atr_feature()\n",
    "        self.add_momentum_volatility_feature()\n",
    "        \n",
    "        #del self.data[\"SPY_pct_change\"]\n",
    "        #del self.data[\"IWM_pct_change\"]\n",
    "        #self.add_2yr_treasury_yield_change_feature(dgs2_filtered)  # Correctly pass dgs2_filtered as an argument\n",
    "\n",
    "        \n",
    "        self.data = self.data.dropna(how = \"any\")\n",
    "        self.label = list(self.data[\"label\"])\n",
    "        del self.data[\"label\"]\n",
    "        del self.data[\"Adj Close\"]\n",
    "        del self.data[\"Open\"]\n",
    "        del self.data[\"High\"]\n",
    "        del self.data[\"Low\"]\n",
    "        \n",
    "        del self.data['avg_volume']\n",
    "        del self.data['vix_pct_change']\n",
    "        del self.data['vix']\n",
    "        #del self.data['macd_signal']\n",
    "        #del self.data['is_uptrend']\n",
    "        #del self.data['is_downtrend']\n",
    "        #del self.data['is_vix_high'] \n",
    "        #del self.data['is_vix_moderate']\n",
    "        #del self.data['is_vix_low']\n",
    "        #del self.data['macd']\n",
    "        print(self.data.columns)\n",
    "\n",
    "\n",
    "    def add_reversal_feature(self, window=5):\n",
    "        \"\"\"\n",
    "        Add reversal feature based on the negative of momentum.\n",
    "        A simple reversal feature could be the negative price change over a given window.\n",
    "        \"\"\"\n",
    "        self.data['reversal_feature'] = -self.data['Adj Close'].pct_change(periods=window)\n",
    "\n",
    "        # Now, make sure to forward fill any NaN values that were generated\n",
    "        self.data['reversal_feature'] = self.data['reversal_feature'].fillna(method=\"ffill\")\n",
    "\n",
    "        # Optionally, rank the reversal features if you are considering a cross-sectional strategy\n",
    "        # self.data['reversal_rank'] = self.data['reversal_feature'].rank(pct=True)\n",
    "\n",
    "        # Normalize the reversal feature so that it's on the same scale as your other features\n",
    "        self.data['reversal_feature'] = (self.data['reversal_feature'] - self.data['reversal_feature'].mean()) / self.data['reversal_feature'].std()\n",
    "\n",
    "    \n",
    "    def price_acceleration(self):\n",
    "        lagged_day = 5\n",
    "        self.data['lagged_data_1'] = self.data['Adj Close'].shift(lagged_day)\n",
    "        self.data['lagged_data_2'] = self.data['Adj Close'].shift(lagged_day * 2)\n",
    "        self.data['price_rate_change_1'] = (self.data['Adj Close'] - self.data['lagged_data_1']) / lagged_day\n",
    "        self.data['price_rate_change_2'] = (self.data['lagged_data_1'] - self.data['lagged_data_2']) / lagged_day\n",
    "        self.data['price_acceleration'] = (self.data['price_rate_change_1'] - self.data['price_rate_change_2']) / lagged_day\n",
    "        del self.data['lagged_data_1']\n",
    "        del self.data['lagged_data_2']\n",
    "        del self.data['price_rate_change_1']\n",
    "        del self.data['price_rate_change_2']\n",
    "\n",
    "    def percent_off_52_weeks_high(self):\n",
    "        one_year_high = self.data['Adj Close'].rolling(window=252).max()\n",
    "        self.data['percent_off_52_weeks_high'] = (one_year_high - self.data['Adj Close']) / one_year_high\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def add_atr_feature(self, window=14):\n",
    "        \"\"\"Add Average True Range (ATR) feature.\"\"\"\n",
    "        high_low = self.data['High'] - self.data['Low']\n",
    "        high_close = np.abs(self.data['High'] - self.data['Adj Close'].shift())\n",
    "        low_close = np.abs(self.data['Low'] - self.data['Adj Close'].shift())\n",
    "\n",
    "        true_ranges = np.maximum(high_low, high_close, low_close)\n",
    "        self.data['atr'] = true_ranges.rolling(window=window).mean()\n",
    "\n",
    "    def add_momentum_volatility_feature(self, momentum_window=14, vol_window=21):\n",
    "        \"\"\"Add Momentum-Based Volatility feature.\"\"\"\n",
    "        momentum = self.data['Adj Close'].pct_change(periods=momentum_window)\n",
    "        self.data['momentum_volatility'] = momentum.rolling(window=vol_window).std()\n",
    "        \n",
    "\n",
    "        \n",
    "    def add_volatility_feature_short(self, window=21):  # 21 trading days roughly equals 1 month\n",
    "        \"\"\"Add Volatility feature calculated as the rolling standard deviation of daily returns.\"\"\"\n",
    "        # Calculate daily returns\n",
    "        daily_returns = self.data['Adj Close'].pct_change()\n",
    "        \n",
    "        # Calculate the rolling standard deviation of daily returns\n",
    "        self.data['volatility_21d'] = daily_returns.rolling(window=window).std() * np.sqrt(window)\n",
    "\n",
    "\n",
    "    def add_volatility_feature_long(self, window=252):  # 21 trading days roughly equals 1 month\n",
    "        \"\"\"Add Volatility feature calculated as the rolling standard deviation of daily returns.\"\"\"\n",
    "        # Calculate daily returns\n",
    "        daily_returns = self.data['Adj Close'].pct_change()\n",
    "        \n",
    "        # Calculate the rolling standard deviation of daily returns\n",
    "        self.data['volatility_252d'] = daily_returns.rolling(window=window).std() * np.sqrt(window)\n",
    "    \n",
    "    def merge_pc_ratio(self, pc_ratio_data):\n",
    "        # Ensure 'DATE' is in datetime format and set it as the index\n",
    "        if 'DATE' in pc_ratio_data.columns:\n",
    "            pc_ratio_data['DATE'] = pd.to_datetime(pc_ratio_data['DATE'])\n",
    "            pc_ratio_data.set_index('DATE', inplace=True)\n",
    "    \n",
    "        # Calculate the P/C Ratio change (you can use pct_change() for percentage change)\n",
    "        pc_ratio_data['PC_Ratio_Change'] = pc_ratio_data['P/C Ratio'].pct_change()\n",
    "    \n",
    "        # Merge the PC_Ratio_Change into self.data\n",
    "        self.data = self.data.join(pc_ratio_data['PC_Ratio_Change'], how='left')\n",
    "    \n",
    "        # Handle any infinite values and fill missing values\n",
    "        self.data['PC_Ratio_Change'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.data['PC_Ratio_Change'] = self.data['PC_Ratio_Change'].fillna(method=\"bfill\")\n",
    "    \n",
    "\n",
    "    def add_momentum_feature_5d(self, window=5):\n",
    "        \"\"\"Add Momentum feature.\"\"\"\n",
    "        self.data['momentum_5d'] = self.data['Adj Close'].pct_change(periods=window).shift(-window)\n",
    "\n",
    "    def add_momentum_feature_10d(self, window=10):\n",
    "        \"\"\"Add Momentum feature.\"\"\"\n",
    "        self.data['momentum_10d'] = self.data['Adj Close'].pct_change(periods=window).shift(-window)\n",
    "\n",
    "    def add_momentum_feature_20d(self, window=20):\n",
    "        \"\"\"Add Momentum feature.\"\"\"\n",
    "        self.data['momentum_10d'] = self.data['Adj Close'].pct_change(periods=window).shift(-window)\n",
    "\n",
    "    def add_momentum_feature_50d(self, window=50):\n",
    "        \"\"\"Add Momentum feature.\"\"\"\n",
    "        self.data['momentum_10d'] = self.data['Adj Close'].pct_change(periods=window).shift(-window)\n",
    "\n",
    "    def add_momentum_feature_200d(self, window=200):\n",
    "        \"\"\"Add Momentum feature.\"\"\"\n",
    "        self.data['momentum_10d'] = self.data['Adj Close'].pct_change(periods=window).shift(-window)\n",
    "               \n",
    "    def add_volume_oscillator(self):\n",
    "            \"\"\"Add Volume Oscillator feature.\"\"\"\n",
    "            short_term = 5\n",
    "            long_term = 10\n",
    "            \n",
    "            self.data['volume_oscillator'] = self.data['Volume'].rolling(window=short_term).mean() - self.data['Volume'].rolling(window=long_term).mean()\n",
    "\n",
    "    def add_relative_volume(self, comparison_period=20):\n",
    "        \"\"\"Add Relative Volume feature.\"\"\"\n",
    "        # Calculate the average volume over the specified comparison period\n",
    "        self.data['avg_volume'] = self.data['Volume'].rolling(window=comparison_period).mean()\n",
    "\n",
    "        # Calculate Relative Volume\n",
    "        self.data['relative_volume'] = self.data['Volume'] / self.data['avg_volume']\n",
    "\n",
    "    def add_volume_relative_to_ma(self, period=50):\n",
    "        \"\"\"Add Volume Relative to Moving Average.\"\"\"\n",
    "        self.data['vol_relative_to_ma'] = self.data['Volume'] / self.data['Volume'].rolling(window=period).mean()\n",
    "\n",
    "    def add_volume_spikes(self, threshold=2):\n",
    "        \"\"\"Add Volume Spikes.\"\"\"\n",
    "        self.data['vol_spike'] = self.data['Volume'] > self.data['Volume'].rolling(window=50).mean() * threshold\n",
    "\n",
    "    def add_price_volume_trend(self):\n",
    "        \"\"\"Add Price-Volume Trend.\"\"\"\n",
    "        self.data['pvt'] = (self.data['Volume'] * self.data['Adj Close'].diff()).cumsum()\n",
    "\n",
    "    def download_vix(self):\n",
    "        \"\"\"Download VIX data for the same date range as the stock data.\"\"\"\n",
    "        start_date = self.data.index.min().strftime('%Y-%m-%d')\n",
    "        end_date = self.data.index.max().strftime('%Y-%m-%d')\n",
    "        self.vix_data = yf.download(\"^VIX\", start=start, end=end)['Close']\n",
    "\n",
    "    def add_vix_feature(self):\n",
    "        \"\"\"Add VIX as a feature along with its percentage change.\"\"\"\n",
    "        self.download_vix()  # Download VIX data\n",
    "\n",
    "        # Merge raw VIX data into the stock data\n",
    "        self.data['vix'] = self.vix_data.reindex(self.data.index, method='bfill')\n",
    "\n",
    "        # Calculate the percentage change in VIX\n",
    "        self.data['vix_pct_change'] = self.data['vix'].pct_change()\n",
    "\n",
    "        # Handle any missing values\n",
    "        self.data['vix_pct_change'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "\n",
    "    def download_spy(self):\n",
    "        \"\"\"Download SPY data for the same date range as the stock data.\"\"\"\n",
    "        start_date = self.data.index.min().strftime('%Y-%m-%d')\n",
    "        end_date = self.data.index.max().strftime('%Y-%m-%d')\n",
    "        self.spy_data = yf.download(\"SPY\", start=start, end=end)['Adj Close']\n",
    "\n",
    "    def IWM_moving_beta_change(self):\n",
    "        \"\"\"Calculate the change in the 30-day rolling beta of IWM relative to SPX.\"\"\"\n",
    "        # Download SPX data\n",
    "        start_date = self.data.index.min().strftime('%Y-%m-%d')\n",
    "        end_date = self.data.index.max().strftime('%Y-%m-%d')\n",
    "        spy_data = yf.download(\"SPY\", start=start_date, end=end_date)['Adj Close']\n",
    "        iwm_data= yf.download(\"IWM\", start=start_date, end=end_date)['Adj Close']\n",
    "        # Calculate returns\n",
    "        iwm_returns = iwm_data.pct_change()\n",
    "        spy_returns = spy_data.pct_change()\n",
    "\n",
    "        # Calculate 30-day rolling beta\n",
    "        covariance = iwm_returns.rolling(window=30).cov(spy_returns)\n",
    "        variance = spy_returns.rolling(window=30).var()\n",
    "        rolling_beta = covariance / variance\n",
    "\n",
    "        # Calculate the change in rolling beta\n",
    "        change_in_rolling_beta = rolling_beta.diff()\n",
    "\n",
    "        # Store the change in rolling beta in the class\n",
    "        self.data['Change_in_Rolling_Beta_IWM'] = change_in_rolling_beta.shift(5)\n",
    "        \n",
    "\n",
    "    def add_spy_vix_ratio_feature(self):\n",
    "        \"\"\"Add feature of SPY price change / VIX price change.\"\"\"\n",
    "        self.download_spy()  # Download SPY data\n",
    "        self.download_vix()  # Download VIX data\n",
    "\n",
    "        # Calculate daily percentage change for SPY and VIX\n",
    "        spy_pct_change = self.spy_data.pct_change()\n",
    "        vix_pct_change = self.vix_data.pct_change()\n",
    "        \n",
    "        # Calculate the ratio of SPY change to VIX change\n",
    "        self.data['SPY_VIX_ratio'] = spy_pct_change/ vix_pct_change\n",
    "        self.data['SPY_VIX_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.data['SPY_VIX_ratio'] = self.data['SPY_VIX_ratio'].fillna(method = \"bfill\")\n",
    "\n",
    "    def download_etf_data(self, ticker, column_name):\n",
    "        \"\"\"Download ETF data for the same date range as the stock data.\"\"\"\n",
    "        start_date = self.data.index.min().strftime('%Y-%m-%d')\n",
    "        end_date = self.data.index.max().strftime('%Y-%m-%d')\n",
    "        etf_data = yf.download(ticker, start=start, end=end)['Adj Close']\n",
    "        etf_pct_change = etf_data.pct_change()\n",
    "        self.data[column_name] = etf_pct_change\n",
    "\n",
    "    def add_spy_iwm_ratio_feature(self):\n",
    "        \"\"\"Add feature of SPY price change / IWM price change.\"\"\"\n",
    "        self.download_etf_data(\"SPY\", \"SPY_pct_change\")\n",
    "        self.download_etf_data(\"IWM\", \"IWM_pct_change\")\n",
    "\n",
    "        # Calculate the ratio of SPY change to IWM change\n",
    "        self.data['SPY_IWM_ratio'] = self.data['SPY_pct_change'] / self.data['IWM_pct_change']\n",
    "        self.data['SPY_IWM_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.data['SPY_IWM_ratio'] = self.data['SPY_VIX_ratio'].fillna(method = \"bfill\")\n",
    "    def add_spy_qqq_ratio_feature(self):\n",
    "        \"\"\"Add feature of SPY price change / QQQ price change.\"\"\"\n",
    "        self.download_etf_data(\"SPY\", \"SPY_pct_change\")\n",
    "        self.download_etf_data(\"QQQ\", \"QQQ_pct_change\")\n",
    "        self.data['SPY_QQQ_ratio'] = self.data['SPY_pct_change'] / self.data['QQQ_pct_change']\n",
    "        self.data['SPY_QQQ_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.data['SPY_QQQ_ratio'] = self.data['SPY_VIX_ratio'].fillna(method = \"bfill\")\n",
    "    def add_spy_dia_ratio_feature(self):\n",
    "        \"\"\"Add feature of SPY price change / DIA price change.\"\"\"\n",
    "        self.download_etf_data(\"SPY\", \"SPY_pct_change\")\n",
    "        self.download_etf_data(\"DIA\", \"DIA_pct_change\")\n",
    "        self.data['SPY_DIA_ratio'] = self.data['SPY_pct_change'] / self.data['DIA_pct_change']\n",
    "        self.data['SPY_DIA_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.data['SPY_DIA_ratio'] = self.data['SPY_VIX_ratio'].fillna(method = \"bfill\")\n",
    "\n",
    "    def add_dxy_feature(self):\n",
    "        \"\"\"Add feature of DXY (US Dollar Index) daily percentage change.\"\"\"\n",
    "    # Download DXY data for the same date range as the stock data\n",
    "\n",
    "        dxy_data = yf.download(\"DX-Y.NYB\", start=start, end=end)['Adj Close']\n",
    "        \n",
    "        # Calculate daily percentage change for DXY\n",
    "        dxy_pct_change = dxy_data.pct_change()\n",
    "        \n",
    "        # Add the DXY daily percentage change to the stock data DataFrame\n",
    "        self.data['DXY_pct_change'] = dxy_pct_change\n",
    "        self.data['DXY_pct_change'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.data['DXY_pct_change'] = self.data['DXY_pct_change'].fillna(method=\"bfill\")\n",
    "    def download_bond_yield_data(self, ticker, column_name):\n",
    "        \"\"\"Download bond yield data for the same date range as the stock data.\"\"\"\n",
    "\n",
    "        bond_data = yf.download(ticker, start=start, end=end)['Adj Close']\n",
    "        bond_pct_change = bond_data.pct_change()\n",
    "        self.data[column_name] = bond_pct_change\n",
    "\n",
    "    def add_2yr_treasury_yield_change_feature(self, dgs2_data):\n",
    "        \"\"\"\n",
    "        Add the 2-year Treasury yield percentage change as a feature to the stock data.\n",
    "        \n",
    "        Args:\n",
    "        dgs2_data (DataFrame): DataFrame containing the DGS2 data with 'DATE' and 'DGS2_pct_change' columns.\n",
    "        \"\"\"\n",
    "        # Check if 'DATE' column exists in dgs2_data\n",
    "        if 'DATE' in dgs2_data.columns:\n",
    "            # If 'DATE' column exists, ensure it's in datetime format and set it as the index\n",
    "            dgs2_data['DATE'] = pd.to_datetime(dgs2_data['DATE'])\n",
    "            dgs2_data.set_index('DATE', inplace=True)\n",
    "        \n",
    "        # Merge the DGS2_pct_change into the stock data\n",
    "        self.data = self.data.join(dgs2_data['DGS2_pct_change'], how='left')\n",
    "\n",
    "        # Handle any infinite values and fill missing values\n",
    "        self.data['DGS2_pct_change'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.data['DGS2_pct_change'] = self.data['DGS2_pct_change'].fillna(method=\"bfill\")\n",
    "    \n",
    "    def add_10yr_bond_yield_change_feature(self):\n",
    "        \"\"\"Add change of 10-year bond yield as a feature.\"\"\"\n",
    "        self.download_bond_yield_data(\"^TNX\", \"10yr_bond_yield_change\")\n",
    "\n",
    "    def add_yield_spread_feature(self, dgs2_data):\n",
    "        \"\"\"\n",
    "        Add the feature representing the difference between 2-year and 10-year Treasury bond yields.\n",
    "        \n",
    "        Args:\n",
    "        dgs2_data (DataFrame): DataFrame containing the local DGS2 data.\n",
    "        \"\"\"\n",
    "        # Download 10-year Treasury yield data\n",
    "        dgs10_data = yf.download(\"^TNX\", start=\"2000-01-01\", end=\"2023-12-20\")['Adj Close']\n",
    "\n",
    "        # Ensure DGS2 data is in the correct format\n",
    "        if 'DATE' in dgs2_data.columns:\n",
    "            dgs2_data.set_index('DATE', inplace=True)\n",
    "        dgs2_data.index = pd.to_datetime(dgs2_data.index)\n",
    "\n",
    "        # Align the DGS10 data with DGS2 data dates\n",
    "        dgs10_aligned = dgs10_data.reindex(dgs2_data.index, method='bfill')\n",
    "\n",
    "        # Calculate the yield spread\n",
    "        yield_spread = dgs10_aligned - dgs2_data['DGS2']\n",
    "\n",
    "        # Add the yield spread to the stock data\n",
    "        self.data['yield_spread'] = yield_spread\n",
    "\n",
    "        # Handle missing values\n",
    "        self.data['yield_spread'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "    def add_rsi_feature(self, window=14):\n",
    "        \"\"\"Add Relative Strength Index (RSI) feature.\"\"\"\n",
    "        delta = self.data['Adj Close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "\n",
    "        rs = gain / loss\n",
    "        self.data['rsi'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    def add_bollinger_bands_feature(self, window=20, num_std=2):\n",
    "        \"\"\"Add Bollinger Bands feature.\"\"\"\n",
    "        rolling_mean = self.data['Adj Close'].rolling(window=window).mean()\n",
    "        rolling_std = self.data['Adj Close'].rolling(window=window).std()\n",
    "\n",
    "        self.data['bollinger_upper'] = rolling_mean + (rolling_std * num_std)\n",
    "        self.data['bollinger_lower'] = rolling_mean - (rolling_std * num_std)\n",
    "\n",
    "    def add_atr_feature(self, window=14):\n",
    "        \"\"\"Add Average True Range (ATR) feature.\"\"\"\n",
    "        high_low = self.data['High'] - self.data['Low']\n",
    "        high_close = np.abs(self.data['High'] - self.data['Adj Close'].shift())\n",
    "        low_close = np.abs(self.data['Low'] - self.data['Adj Close'].shift())\n",
    "\n",
    "        ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
    "        true_range = np.max(ranges, axis=1)\n",
    "        self.data['atr'] = true_range.rolling(window=window).mean()\n",
    "\n",
    "    def add_stochastic_oscillator(self, k_window=14, d_window=3):\n",
    "        \"\"\"Add Stochastic Oscillator feature.\"\"\"\n",
    "        min_low = self.data['Low'].rolling(window=k_window).min()\n",
    "        max_high = self.data['High'].rolling(window=k_window).max()\n",
    "\n",
    "        self.data['%K'] = 100 * ((self.data['Adj Close'] - min_low) / (max_high - min_low))\n",
    "        self.data['%D'] = self.data['%K'].rolling(window=d_window).mean()\n",
    "\n",
    "    def add_cci_feature(self, window=20):\n",
    "        \"\"\"Add Commodity Channel Index (CCI) feature.\"\"\"\n",
    "        tp = (self.data['High'] + self.data['Low'] + self.data['Adj Close']) / 3\n",
    "        cci = (tp - tp.rolling(window=window).mean()) / (0.015 * tp.rolling(window=window).std())\n",
    "        self.data['cci'] = cci\n",
    "\n",
    "    def add_obv_feature(self):\n",
    "        \"\"\"Add On-Balance Volume (OBV) feature.\"\"\"\n",
    "        obv = (np.sign(self.data['Adj Close'].diff()) * self.data['Volume']).fillna(0).cumsum()\n",
    "        self.data['obv'] = obv\n",
    "\n",
    "    def calculate_macd(self):\n",
    "        # Calculate MACD\n",
    "        ema_short = self.data['Adj Close'].ewm(span=12, adjust=False, min_periods=12).mean()\n",
    "        ema_long = self.data['Adj Close'].ewm(span=26, adjust=False, min_periods=26).mean()\n",
    "        self.data['macd'] = ema_short - ema_long\n",
    "        self.data['macd_signal'] = self.data['macd'].ewm(span=9, adjust=False, min_periods=9).mean()\n",
    "        \n",
    "    def classify_trend(self):\n",
    "        self.calculate_macd()\n",
    "        self.data['is_uptrend'] = (self.data['macd'] > self.data['macd_signal']).astype(int)\n",
    "        self.data['is_downtrend'] = (self.data['macd'] < self.data['macd_signal']).astype(int)\n",
    "\n",
    "    def add_vix_categories(self):\n",
    "        vix_80_percentile = self.data['vix'].quantile(0.8)\n",
    "        vix_40_percentile = self.data['vix'].quantile(0.4)\n",
    "        self.data['is_vix_high'] = (self.data['vix'] > 30).astype(int)\n",
    "        self.data['is_vix_moderate'] = ((self.data['vix'] >= 20) & (self.data['vix'] <= 30)).astype(int)\n",
    "        self.data['is_vix_low'] = (self.data['vix'] < 20).astype(int)\n",
    "    \n",
    "    def create_combined_conditions(self):\n",
    "        self.classify_trend()\n",
    "        self.add_vix_categories()\n",
    "    \n",
    "        self.data['up_high'] = (self.data['is_uptrend'] & self.data['is_vix_high']).astype(int)\n",
    "        self.data['down_high'] = (self.data['is_downtrend'] & self.data['is_vix_high']).astype(int)\n",
    "        self.data['up_moderate'] = (self.data['is_uptrend'] & self.data['is_vix_moderate']).astype(int)\n",
    "        self.data['down_moderate'] = (self.data['is_downtrend'] & self.data['is_vix_moderate']).astype(int)\n",
    "        self.data['up_low'] = (self.data['is_uptrend'] & self.data['is_vix_low']).astype(int)\n",
    "        self.data['down_low'] = (self.data['is_downtrend'] & self.data['is_vix_low']).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "    #standardize data\n",
    "    def standardize(self):\n",
    "        scaler = StandardScaler()      \n",
    "        self.data = scaler.fit_transform(self.data)\n",
    "\n",
    "    \n",
    "    #normalize data\n",
    "    def normalize(self):\n",
    "        scaler = MinMaxScaler()      \n",
    "        self.data = scaler.fit_transform(self.data)\n",
    "  \n",
    "    # Assuming self.data is a pandas DataFrame and self.label is the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Prepare the Data\n",
    "Step 2: Normalize and Standardize the Data\n",
    "Step 3: Create Sequences\n",
    "Step 4: Split the Data into Training and Testing sets\n",
    "Step 5: Build and Train LSTM model \n",
    "Step 6: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Volume', 'volume_pct_change', 'vol_ma_5', 'vol_ma_20', 'vol_ma_50',\n",
      "       'vol_ma_200', 'ma_5', 'ma_10', 'ma_20', 'ma_50', 'ma_200',\n",
      "       'PC_Ratio_Change', 'volume_oscillator', 'relative_volume',\n",
      "       'vol_relative_to_ma', 'pvt', 'SPY_VIX_ratio', 'rsi', 'bollinger_upper',\n",
      "       'bollinger_lower', 'cci', '10yr_bond_yield_change', 'yield_spread',\n",
      "       'atr', '%K', '%D', 'SPY_pct_change', 'IWM_pct_change', 'SPY_IWM_ratio',\n",
      "       'QQQ_pct_change', 'SPY_QQQ_ratio', 'DIA_pct_change', 'SPY_DIA_ratio',\n",
      "       'DXY_pct_change', 'obv', 'momentum_5d', 'volatility_252d',\n",
      "       'momentum_volatility'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "yf_data = yf.download(\"SPY\", start=start, end=end)\n",
    "stock = Stock(yf_data)\n",
    "stock.factor()\n",
    "\n",
    "# Standardize the data\n",
    "stock.standardize()\n",
    "\n",
    "# Normalize the data\n",
    "stock.normalize()\n",
    "\n",
    "def create_sequence(data, label, time_steps=1) :\n",
    "    Xs, ys = [], []\n",
    "\n",
    "    for i in range(len(data) - time_steps) :\n",
    "        Xs.append(data[i:(i + time_steps)])\n",
    "        ys.append(label[i:(i + time_steps)])\n",
    "\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "time_steps = 60 # adjust based on temporal frame\n",
    "\n",
    "X, y = create_sequence(stock.data, stock.label, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 02:14:34.042475: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-02-27 02:14:34.043237: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-02-27 02:14:34.043730: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-02-27 02:14:34.116403: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-02-27 02:14:34.117059: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-02-27 02:14:34.117486: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 02:14:34.171406: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-02-27 02:14:34.287680: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-02-27 02:14:34.288363: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-02-27 02:14:34.289094: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-02-27 02:14:34.369626: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-02-27 02:14:34.370464: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-02-27 02:14:34.370987: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-02-27 02:14:34.603525: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-02-27 02:14:34.604034: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-02-27 02:14:34.604731: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-02-27 02:14:34.681817: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-02-27 02:14:34.682635: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-02-27 02:14:34.683205: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/65 [===========================>..] - ETA: 0s - loss: 0.2569"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 02:14:36.324972: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-02-27 02:14:36.325410: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-02-27 02:14:36.325834: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-02-27 02:14:36.403672: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-02-27 02:14:36.404118: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-02-27 02:14:36.404630: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 2s 22ms/step - loss: 0.2564 - val_loss: 0.2526\n",
      "Epoch 2/100\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 0.2463 - val_loss: 0.2486\n",
      "Epoch 3/100\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 0.2459 - val_loss: 0.2495\n",
      "Epoch 4/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2457 - val_loss: 0.2497\n",
      "Epoch 5/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2458 - val_loss: 0.2485\n",
      "Epoch 6/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2454 - val_loss: 0.2482\n",
      "Epoch 7/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2453 - val_loss: 0.2494\n",
      "Epoch 8/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2451 - val_loss: 0.2485\n",
      "Epoch 9/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2450 - val_loss: 0.2489\n",
      "Epoch 10/100\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 0.2451 - val_loss: 0.2486\n",
      "Epoch 11/100\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 0.2451 - val_loss: 0.2490\n",
      "Epoch 12/100\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 0.2448 - val_loss: 0.2489\n",
      "Epoch 13/100\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 0.2448 - val_loss: 0.2484\n",
      "Epoch 14/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2446 - val_loss: 0.2492\n",
      "Epoch 15/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2446 - val_loss: 0.2489\n",
      "Epoch 16/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2446 - val_loss: 0.2491\n",
      "Epoch 17/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2447 - val_loss: 0.2490\n",
      "Epoch 18/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2445 - val_loss: 0.2492\n",
      "Epoch 19/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2446 - val_loss: 0.2490\n",
      "Epoch 20/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2445 - val_loss: 0.2484\n",
      "Epoch 21/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2444 - val_loss: 0.2487\n",
      "Epoch 22/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2444 - val_loss: 0.2483\n",
      "Epoch 23/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2444 - val_loss: 0.2484\n",
      "Epoch 24/100\n",
      "65/65 [==============================] - 1s 20ms/step - loss: 0.2444 - val_loss: 0.2480\n",
      "Epoch 25/100\n",
      "65/65 [==============================] - 1s 20ms/step - loss: 0.2444 - val_loss: 0.2493\n",
      "Epoch 26/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2443 - val_loss: 0.2490\n",
      "Epoch 27/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2444 - val_loss: 0.2494\n",
      "Epoch 28/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2443 - val_loss: 0.2488\n",
      "Epoch 29/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2444 - val_loss: 0.2500\n",
      "Epoch 30/100\n",
      "65/65 [==============================] - 1s 22ms/step - loss: 0.2443 - val_loss: 0.2482\n",
      "Epoch 31/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2444 - val_loss: 0.2501\n",
      "Epoch 32/100\n",
      "65/65 [==============================] - 1s 20ms/step - loss: 0.2444 - val_loss: 0.2495\n",
      "Epoch 33/100\n",
      "65/65 [==============================] - 1s 21ms/step - loss: 0.2443 - val_loss: 0.2482\n",
      "Epoch 34/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2443 - val_loss: 0.2491\n",
      "Epoch 35/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2443 - val_loss: 0.2498\n",
      "Epoch 36/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2442 - val_loss: 0.2494\n",
      "Epoch 37/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2442 - val_loss: 0.2489\n",
      "Epoch 38/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2442 - val_loss: 0.2489\n",
      "Epoch 39/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2442 - val_loss: 0.2511\n",
      "Epoch 40/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2442 - val_loss: 0.2497\n",
      "Epoch 41/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2443 - val_loss: 0.2514\n",
      "Epoch 42/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2442 - val_loss: 0.2495\n",
      "Epoch 43/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2443 - val_loss: 0.2509\n",
      "Epoch 44/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2442 - val_loss: 0.2485\n",
      "Epoch 45/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2443 - val_loss: 0.2487\n",
      "Epoch 46/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2442 - val_loss: 0.2490\n",
      "Epoch 47/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2443 - val_loss: 0.2512\n",
      "Epoch 48/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2442 - val_loss: 0.2498\n",
      "Epoch 49/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2442 - val_loss: 0.2509\n",
      "Epoch 50/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2442 - val_loss: 0.2491\n",
      "Epoch 51/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2442 - val_loss: 0.2497\n",
      "Epoch 52/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2442 - val_loss: 0.2499\n",
      "Epoch 53/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2442 - val_loss: 0.2507\n",
      "Epoch 54/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2442 - val_loss: 0.2495\n",
      "Epoch 55/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2441 - val_loss: 0.2498\n",
      "Epoch 56/100\n",
      "65/65 [==============================] - 1s 20ms/step - loss: 0.2442 - val_loss: 0.2484\n",
      "Epoch 57/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2442 - val_loss: 0.2503\n",
      "Epoch 58/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2441 - val_loss: 0.2528\n",
      "Epoch 59/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2531\n",
      "Epoch 60/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2506\n",
      "Epoch 61/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2524\n",
      "Epoch 62/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2498\n",
      "Epoch 63/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2513\n",
      "Epoch 64/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2500\n",
      "Epoch 65/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2441 - val_loss: 0.2499\n",
      "Epoch 66/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2505\n",
      "Epoch 67/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2503\n",
      "Epoch 68/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2503\n",
      "Epoch 69/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2498\n",
      "Epoch 70/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2442 - val_loss: 0.2508\n",
      "Epoch 71/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2507\n",
      "Epoch 72/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2539\n",
      "Epoch 73/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2442 - val_loss: 0.2499\n",
      "Epoch 74/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2504\n",
      "Epoch 75/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2522\n",
      "Epoch 76/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2512\n",
      "Epoch 77/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2522\n",
      "Epoch 78/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2442 - val_loss: 0.2507\n",
      "Epoch 79/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2442 - val_loss: 0.2515\n",
      "Epoch 80/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2442 - val_loss: 0.2521\n",
      "Epoch 81/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2493\n",
      "Epoch 82/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2511\n",
      "Epoch 83/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2513\n",
      "Epoch 84/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2497\n",
      "Epoch 85/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2527\n",
      "Epoch 86/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2441 - val_loss: 0.2515\n",
      "Epoch 87/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2505\n",
      "Epoch 88/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2510\n",
      "Epoch 89/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2498\n",
      "Epoch 90/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2503\n",
      "Epoch 91/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2509\n",
      "Epoch 92/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2499\n",
      "Epoch 93/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2518\n",
      "Epoch 94/100\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.2442 - val_loss: 0.2507\n",
      "Epoch 95/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2511\n",
      "Epoch 96/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2501\n",
      "Epoch 97/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2509\n",
      "Epoch 98/100\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2441 - val_loss: 0.2510\n",
      "Epoch 99/100\n",
      "65/65 [==============================] - 1s 20ms/step - loss: 0.2440 - val_loss: 0.2499\n",
      "Epoch 100/100\n",
      "65/65 [==============================] - 1s 20ms/step - loss: 0.2441 - val_loss: 0.2496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x286bfc810>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train-test split\n",
    "train_size = int(len(X) * 0.8)\n",
    "\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Define the LSTM model\n",
    "model = keras.Sequential()\n",
    "model.add(layers.LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(layers.LSTM(50, return_sequences=False))\n",
    "model.add(layers.Dense(25))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=100, validation_split=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "6468b8bafc3c92ec413f9e993d876a954e3f3f9c76b8a1a2cbbcabcc87c80098"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
