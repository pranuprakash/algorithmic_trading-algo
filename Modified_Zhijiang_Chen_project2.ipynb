{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6211c439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/pranuprakash/Library/Mobile Documents/com~apple~CloudDocs/pranup/Algorithmic Training/Project', '/', '/Users/pranuprakash/.vscode/extensions/ms-toolsai.jupyter-2022.8.1002431955/pythonFiles', '/Users/pranuprakash/.vscode/extensions/ms-toolsai.jupyter-2022.8.1002431955/pythonFiles/lib/python', '/Users/pranuprakash/opt/anaconda3/lib/python39.zip', '/Users/pranuprakash/opt/anaconda3/lib/python3.9', '/Users/pranuprakash/opt/anaconda3/lib/python3.9/lib-dynload', '', '/Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages', '/Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages/IPython/extensions', '/Users/pranuprakash/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1583d3c4-102f-44ba-ae03-fd7dc502e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import backtrader as bt\n",
    "import quantstats as qs\n",
    "import pyfolio as pf\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "719e4759-03a1-43f7-a4ee-effafb066867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Stock:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.label = []\n",
    "    \n",
    "    #preprocess\n",
    "    #feature creation\n",
    "    def factor(self):\n",
    "        del self.data[\"Close\"]\n",
    "        self.data = self.data.fillna(method = \"bfill\")\n",
    "        \n",
    "        #return\n",
    "        #>0,+1;<=0,-1\n",
    "        self.data['label']=self.data.rolling(2).apply(lambda x:x.iloc[1]>x.iloc[0])['Adj Close']\n",
    "        self.data['label']=self.data.label.shift(-1)\n",
    "        \n",
    "        self.data['volume'] = self.data['Volume']\n",
    "        self.data['volume_change'] = self.data['Volume'].diff()\n",
    "\n",
    "        # Volume Change (daily percentage change)\n",
    "        self.data['volume_pct_change'] = self.data['Volume'].pct_change()\n",
    "\n",
    "        # Volume Moving Averages\n",
    "        self.data['vol_ma_5'] = self.data['Volume'].rolling(window=5).mean()\n",
    "        self.data['vol_ma_10'] = self.data['Volume'].rolling(window=10).mean()\n",
    "        self.data['vol_ma_20'] = self.data['Volume'].rolling(window=20).mean()  \n",
    "        self.data['vol_ma_50'] = self.data['Volume'].rolling(window=50).mean()  \n",
    "        self.data['vol_ma_200'] = self.data['Volume'].rolling(window=200).mean()  \n",
    "\n",
    "        #moving average\n",
    "        self.data[\"ma_5\"] = self.data[\"Adj Close\"].rolling(window = 5).mean()\n",
    "        self.data[\"ma_10\"] = self.data[\"Adj Close\"].rolling(window = 10).mean()\n",
    "        self.data[\"ma_20\"] = self.data[\"Adj Close\"].rolling(window = 20).mean()\n",
    "        self.data[\"ma_50\"] = self.data[\"Adj Close\"].rolling(window = 50).mean()\n",
    "        self.data[\"ma_200\"] = self.data[\"Adj Close\"].rolling(window = 200).mean()\n",
    "\n",
    "        self.add_volume_oscillator()\n",
    "        self.add_relative_volume()\n",
    "        self.add_volume_relative_to_ma()\n",
    "        self.add_volume_spikes()\n",
    "        self.add_price_volume_trend()\n",
    "        #self.add_vix_feature()\n",
    "        self.add_spy_vix_ratio_feature()\n",
    "        self.add_spy_iwm_ratio_feature()\n",
    "        #self.add_spy_qqq_ratio_feature()\n",
    "        #self.add_spy_dia_ratio_feature()\n",
    "        #macd\n",
    "        ema_short = self.data['Adj Close'].ewm(span=12, adjust=False, min_periods=12).mean()\n",
    "        ema_long = self.data['Adj Close'].ewm(span=26, adjust=False, min_periods=26).mean()\n",
    "        macd = ema_short - ema_long\n",
    "        macd_s = macd.ewm(span=9, adjust=False, min_periods=9).mean()\n",
    "        macd_h = macd - macd_s\n",
    "        self.data['macd'] = macd\n",
    "        self.data['macd_h'] = macd_h\n",
    "        self.data['macd_s'] = macd_s\n",
    "        \n",
    "        self.data = self.data.dropna(how = \"any\")\n",
    "        self.label = list(self.data[\"label\"])\n",
    "        del self.data[\"label\"]\n",
    "        del self.data[\"Adj Close\"]\n",
    "               \n",
    "    def add_volume_oscillator(self):\n",
    "        \"\"\"Add Volume Oscillator feature.\"\"\"\n",
    "        short_term = 5\n",
    "        long_term = 10\n",
    "        self.data['vol_ma_short'] = self.data['Volume'].rolling(window=short_term).mean()\n",
    "        self.data['vol_ma_long'] = self.data['Volume'].rolling(window=long_term).mean()\n",
    "        self.data['volume_oscillator'] = self.data['vol_ma_short'] - self.data['vol_ma_long']\n",
    "\n",
    "    def add_relative_volume(self, comparison_period=20):\n",
    "        \"\"\"Add Relative Volume feature.\"\"\"\n",
    "        # Calculate the average volume over the specified comparison period\n",
    "        self.data['avg_volume'] = self.data['Volume'].rolling(window=comparison_period).mean()\n",
    "\n",
    "        # Calculate Relative Volume\n",
    "        self.data['relative_volume'] = self.data['Volume'] / self.data['avg_volume']\n",
    "\n",
    "    def add_volume_relative_to_ma(self, period=50):\n",
    "        \"\"\"Add Volume Relative to Moving Average.\"\"\"\n",
    "        self.data['vol_relative_to_ma'] = self.data['Volume'] / self.data['Volume'].rolling(window=period).mean()\n",
    "\n",
    "    def add_volume_spikes(self, threshold=2):\n",
    "        \"\"\"Add Volume Spikes.\"\"\"\n",
    "        self.data['vol_spike'] = self.data['Volume'] > self.data['Volume'].rolling(window=50).mean() * threshold\n",
    "\n",
    "    def add_price_volume_trend(self):\n",
    "        \"\"\"Add Price-Volume Trend.\"\"\"\n",
    "        self.data['pvt'] = (self.data['Volume'] * self.data['Adj Close'].diff()).cumsum()\n",
    "\n",
    "    def download_vix(self):\n",
    "        \"\"\"Download VIX data for the same date range as the stock data.\"\"\"\n",
    "        start_date = self.data.index.min().strftime('%Y-%m-%d')\n",
    "        end_date = self.data.index.max().strftime('%Y-%m-%d')\n",
    "        self.vix_data = yf.download(\"^VIX\", start=start_date, end=end_date)['Close']\n",
    "\n",
    "    def add_vix_feature(self):\n",
    "        \"\"\"Add VIX as a feature.\"\"\"\n",
    "        self.download_vix()  # Download VIX data\n",
    "        # Merge VIX data into the stock data\n",
    "        self.data = self.data.merge(self.vix_data, how='left', left_index=True, right_index=True, suffixes=('', '_VIX'))\n",
    "        self.data.rename(columns={'Close_VIX': 'VIX'}, inplace=True)\n",
    "\n",
    "    def download_spy(self):\n",
    "        \"\"\"Download SPY data for the same date range as the stock data.\"\"\"\n",
    "        start_date = self.data.index.min().strftime('%Y-%m-%d')\n",
    "        end_date = self.data.index.max().strftime('%Y-%m-%d')\n",
    "        self.spy_data = yf.download(\"SPY\", start=start_date, end=end_date)['Adj Close']\n",
    "\n",
    "    def add_spy_vix_ratio_feature(self):\n",
    "        \"\"\"Add feature of SPY price change / VIX price change.\"\"\"\n",
    "        self.download_spy()  # Download SPY data\n",
    "        self.download_vix()  # Download VIX data\n",
    "\n",
    "        # Calculate daily percentage change for SPY and VIX\n",
    "        spy_pct_change = self.spy_data.pct_change()\n",
    "        vix_pct_change = self.vix_data.pct_change()\n",
    "        \n",
    "        # Calculate the ratio of SPY change to VIX change\n",
    "        self.data['SPY_VIX_ratio'] = spy_pct_change/ vix_pct_change\n",
    "        self.data['SPY_VIX_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.data['SPY_VIX_ratio'] = self.data['SPY_VIX_ratio'].fillna(method = \"bfill\")\n",
    "\n",
    "    def download_etf_data(self, ticker, column_name):\n",
    "        \"\"\"Download ETF data for the same date range as the stock data.\"\"\"\n",
    "        start_date = self.data.index.min().strftime('%Y-%m-%d')\n",
    "        end_date = self.data.index.max().strftime('%Y-%m-%d')\n",
    "        etf_data = yf.download(ticker, start=start_date, end=end_date)['Adj Close']\n",
    "        etf_pct_change = etf_data.pct_change()\n",
    "        self.data[column_name] = etf_pct_change\n",
    "\n",
    "    def add_spy_iwm_ratio_feature(self):\n",
    "        \"\"\"Add feature of SPY price change / IWM price change.\"\"\"\n",
    "        self.download_etf_data(\"SPY\", \"SPY_pct_change\")\n",
    "        self.download_etf_data(\"IWM\", \"IWM_pct_change\")\n",
    "\n",
    "        # Calculate the ratio of SPY change to IWM change\n",
    "        self.data['SPY_IWM_ratio'] = self.data['SPY_pct_change'] / self.data['IWM_pct_change']\n",
    "        self.data['SPY_IWM_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.data['SPY_IWM_ratio'] = self.data['SPY_VIX_ratio'].fillna(method = \"bfill\")\n",
    "    def add_spy_qqq_ratio_feature(self):\n",
    "        \"\"\"Add feature of SPY price change / QQQ price change.\"\"\"\n",
    "        self.download_etf_data(\"SPY\", \"SPY_pct_change\")\n",
    "        self.download_etf_data(\"QQQ\", \"QQQ_pct_change\")\n",
    "        self.data['SPY_QQQ_ratio'] = self.data['SPY_pct_change'] / self.data['QQQ_pct_change']\n",
    "        self.data['SPY_QQQ_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.data['SPY_QQQ_ratio'] = self.data['SPY_VIX_ratio'].fillna(method = \"bfill\")\n",
    "    def add_spy_dia_ratio_feature(self):\n",
    "        \"\"\"Add feature of SPY price change / DIA price change.\"\"\"\n",
    "        self.download_etf_data(\"SPY\", \"SPY_pct_change\")\n",
    "        self.download_etf_data(\"DIA\", \"DIA_pct_change\")\n",
    "        self.data['SPY_DIA_ratio'] = self.data['SPY_pct_change'] / self.data['DIA_pct_change']\n",
    "        self.data['SPY_DIA_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.data['SPY_DIA_ratio'] = self.data['SPY_VIX_ratio'].fillna(method = \"bfill\")\n",
    "\n",
    "    #standardize data\n",
    "    \n",
    "    # def standardize(self):\n",
    "    #     scaler = StandardScaler()      \n",
    "    #     self.data = scaler.fit_transform(self.data)\n",
    "\n",
    "    def standardize(self):\n",
    "        # Replace infinite values with NaN\n",
    "        self.data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "        # Optionally, fill NaN values with column means or another strategy\n",
    "        self.data.fillna(self.data.mean(), inplace=True)\n",
    "\n",
    "        # Now standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        self.data = scaler.fit_transform(self.data)\n",
    "    \n",
    "    #normalize data\n",
    "    def normalize(self):\n",
    "        scaler = MinMaxScaler()      \n",
    "        self.data = scaler.fit_transform(self.data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "699943e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "accuracy_xgb:  0.5266383781235267\n",
      "precision_xgb:  0.5561005518087063\n",
      "Confusion Matrix:\n",
      " [[210 724]\n",
      " [280 907]]\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Assuming Stock class and list1 are defined as before\n",
    "\n",
    "# Lists to store combined features and labels for all stocks\n",
    "all_features_xgb = []\n",
    "all_labels_xgb = []\n",
    "\n",
    "# Download and preprocess data for each ticker in the list\n",
    "yf_data = yf.download(\"SPY\", start=\"2000-01-01\", end=\"2021-11-12\")\n",
    "stock = Stock(yf_data)\n",
    "stock.factor()        # Feature creation and preprocessing\n",
    "stock.standardize()   # Standardizing the data\n",
    "stock.normalize()     # Normalizing the data\n",
    "\n",
    "# Append the features and labels to the respective lists\n",
    "all_features_xgb.append(pd.DataFrame(stock.data))\n",
    "all_labels_xgb.extend(stock.label)\n",
    "\n",
    "X = pd.concat(all_features_xgb, ignore_index=True)\n",
    "y = pd.Series(all_labels_xgb)\n",
    "\n",
    "# Set a fixed random state for reproducibility\n",
    "fixed_random_state = 42\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=fixed_random_state,shuffle=False)\n",
    "\n",
    "# Initialize an XGBoost classifier with adjusted parameters\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    max_depth=6,\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=fixed_random_state\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "pred_y = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculating accuracy and precision\n",
    "accuracy_xgb = accuracy_score(y_test, pred_y)\n",
    "precision_xgb = precision_score(y_test, pred_y)\n",
    "print(\"accuracy_xgb: \", accuracy_xgb)\n",
    "print(\"precision_xgb: \", precision_xgb)\n",
    "cm = confusion_matrix(y_test, pred_y)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "195208ab-8cdf-40ff-b98f-0e46ba22d8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "Cross-Validation Accuracy Scores: [0.53793884 0.53227633 0.45526614 0.5198188  0.53907135]\n",
      "Mean CV Accuracy: 0.5168742921857304\n",
      "Cross-Validation Precision Scores: [0.56825397 0.55379747 0.54491018 0.55936073 0.58277027]\n",
      "Mean CV Precision: 0.561818523422599\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate, TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "import xgboost as xgb\n",
    "\n",
    "# Assuming the Stock class is defined as before\n",
    "\n",
    "# Download and preprocess data\n",
    "yf_data = yf.download(\"SPY\", start=\"2000-01-01\", end=\"2021-11-12\")\n",
    "stock = Stock(yf_data)\n",
    "stock.factor()        # Feature creation and preprocessing\n",
    "stock.standardize()   # Standardizing the data\n",
    "stock.normalize()     # Normalizing the data\n",
    "\n",
    "# Create feature matrix X and target vector y\n",
    "X = pd.DataFrame(stock.data)\n",
    "y = pd.Series(stock.label)\n",
    "\n",
    "# Initialize an XGBoost classifier with adjusted parameters\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    max_depth=6,\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=fixed_random_state\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Define the number of folds for cross-validation and scoring metrics\n",
    "num_folds = 5\n",
    "scoring_metrics = {'accuracy': make_scorer(accuracy_score), \n",
    "                   'precision': make_scorer(precision_score)}\n",
    "\n",
    "# Use TimeSeriesSplit for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=num_folds)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate(xgb_model, X, y, cv=tscv, scoring=scoring_metrics, return_train_score=True)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Cross-Validation Accuracy Scores: {cv_results['test_accuracy']}\")\n",
    "print(f\"Mean CV Accuracy: {cv_results['test_accuracy'].mean()}\")\n",
    "print(f\"Cross-Validation Precision Scores: {cv_results['test_precision']}\")\n",
    "print(f\"Mean CV Precision: {cv_results['test_precision'].mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a060e6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "accuracy_rf:  0.4926921263554927\n",
      "precision_rf:  0.5459817729908865\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Ensure this file exists in your directory\n",
    "list1 = {\"SPY\"}\n",
    "# Assuming Stock class and list1 are defined as before\n",
    "\n",
    "# Lists to store combined features and labels for all stocks\n",
    "all_features_rf = []\n",
    "all_labels_rf = []\n",
    "\n",
    "for tic in list1:\n",
    "    # Download and preprocess data for each ticker in the list\n",
    "    yf_data = yf.download(tic, start=\"2000-01-01\", end=\"2021-11-12\")\n",
    "    \n",
    "    # Skip if data is empty\n",
    "    if yf_data.empty:\n",
    "        continue\n",
    "\n",
    "    stock = Stock(yf_data)\n",
    "    stock.factor()        # Feature creation and preprocessing\n",
    "    stock.standardize()   # Standardizing the data\n",
    "    stock.normalize() \n",
    "\n",
    "    # Append the features and labels to the respective lists\n",
    "    all_features_rf.append(pd.DataFrame(stock.data))\n",
    "    all_labels_rf.extend(stock.label)\n",
    "\n",
    "# Concatenating all features and labels into single datasets\n",
    "X = pd.concat(all_features_rf)\n",
    "y = pd.Series(all_labels_rf)\n",
    "\n",
    "# Set a fixed random state for reproducibility\n",
    "fixed_random_state = 42\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=fixed_random_state,shuffle=False)\n",
    "\n",
    "# Initialize a Random Forest classifier with adjusted parameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=150,    # Increased number of trees\n",
    "    max_depth=10,        # Maximum depth of each tree\n",
    "    min_samples_split=4, # Minimum number of samples required to split an internal node\n",
    "    min_samples_leaf=2,  # Minimum number of samples required to be at a leaf node\n",
    "    random_state=fixed_random_state  # Random state for reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "pred_y_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Calculating accuracy and precision for the combined dataset\n",
    "accuracy_rf = accuracy_score(y_test, pred_y_rf)\n",
    "precision_rf = precision_score(y_test, pred_y_rf)\n",
    "print(\"accuracy_rf: \", accuracy_rf)\n",
    "print(\"precision_rf: \", precision_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91371e9c-b34f-4545-87f0-07033fd74283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "Cross-Validation Accuracy Scores: [0.47678369 0.53907135 0.46092865 0.49490374 0.4801812 ]\n",
      "Mean CV Accuracy: 0.49037372593431494\n",
      "Cross-Validation Precision Scores: [0.56299213 0.55409357 0.54716981 0.61029412 0.58273381]\n",
      "Mean CV Precision: 0.5714566870306336\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate, TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Ensure this file exists in your directory\n",
    "list1 = {\"SPY\"}\n",
    "# Assuming Stock class and list1 are defined as before\n",
    "\n",
    "# Lists to store combined features and labels for all stocks\n",
    "all_features_rf = []\n",
    "all_labels_rf = []\n",
    "\n",
    "for tic in list1:\n",
    "    # Download and preprocess data for each ticker in the list\n",
    "    yf_data = yf.download(tic, start=\"2000-01-01\", end=\"2021-11-12\")\n",
    "    \n",
    "    # Skip if data is empty\n",
    "    if yf_data.empty:\n",
    "        continue\n",
    "\n",
    "    stock = Stock(yf_data)\n",
    "    stock.factor()        # Feature creation and preprocessing\n",
    "    stock.standardize()   # Standardizing the data\n",
    "    stock.normalize()     # Normalizing the data\n",
    "\n",
    "    # Append the features and labels to the respective lists\n",
    "    all_features_rf.append(pd.DataFrame(stock.data))\n",
    "    all_labels_rf.extend(stock.label)\n",
    "\n",
    "# Concatenating all features and labels into single datasets\n",
    "X = pd.concat(all_features_rf)\n",
    "y = pd.Series(all_labels_rf)\n",
    "\n",
    "# Initialize a Random Forest classifier with adjusted parameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=150,    # Increased number of trees\n",
    "    max_depth=10,        # Maximum depth of each tree\n",
    "    min_samples_split=4, # Minimum number of samples required to split an internal node\n",
    "    min_samples_leaf=2,  # Minimum number of samples required to be at a leaf node\n",
    "    random_state=42      # Random state for reproducibility\n",
    ")\n",
    "\n",
    "# Fitting the rf_model to x_training and y_training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Define the number of folds for cross-validation and scoring metrics\n",
    "num_folds = 5\n",
    "scoring_metrics = {'accuracy': make_scorer(accuracy_score), \n",
    "                   'precision': make_scorer(precision_score)}\n",
    "\n",
    "# Use TimeSeriesSplit for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=num_folds)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate(rf_model, X, y, cv=tscv, scoring=scoring_metrics, return_train_score=True)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Cross-Validation Accuracy Scores: {cv_results['test_accuracy']}\")\n",
    "print(f\"Mean CV Accuracy: {cv_results['test_accuracy'].mean()}\")\n",
    "print(f\"Cross-Validation Precision Scores: {cv_results['test_precision']}\")\n",
    "print(f\"Mean CV Precision: {cv_results['test_precision'].mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "270d9fa0-9845-4197-bc65-ee8e6884448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = pd.read_csv(\"tickers.csv\")  # Ensure this file exists in your directory\n",
    "list1 = list(ticker[\"Ticker\"])\n",
    "# Custom data loading class for backtrader\n",
    "class PredictionsData(bt.feeds.PandasData):\n",
    "    lines = ('predictions',)\n",
    "    params = (('predictions', -1),)\n",
    "\n",
    "def get_stock_predictions(ticker, model):\n",
    "    yf_data = yf.download(ticker, start=\"2000-01-01\", end=\"2021-11-12\")\n",
    "    if yf_data.empty:\n",
    "        return None\n",
    "\n",
    "    # Preprocess the data\n",
    "    stock = Stock(yf_data.copy())\n",
    "    stock.factor()\n",
    "    stock.standardize()\n",
    "    stock.normalize()\n",
    "\n",
    "    # Splitting the processed data\n",
    "    X = pd.DataFrame(stock.data)\n",
    "    _, X_test = train_test_split(X, test_size=0.4, random_state=42)\n",
    "\n",
    "    # Generate predictions for the test set\n",
    "    test_predictions = model.predict(X_test)\n",
    "\n",
    "    # Store predictions in a DataFrame with the same index as the original data\n",
    "    predictions_series = pd.Series(index=yf_data.index)\n",
    "    predictions_series[X_test.index] = test_predictions\n",
    "    yf_data['predictions'] = predictions_series\n",
    "\n",
    "    return yf_data\n",
    "\n",
    "# Backtrader strategy class\n",
    "class RFStrategy(bt.Strategy):\n",
    "    def __init__(self):\n",
    "        self.predicted = self.datas[0].predictions\n",
    "\n",
    "    def next(self):\n",
    "        if not self.position:\n",
    "            if self.predicted[0] == 1 and self.broker.get_cash() > 100:\n",
    "                self.buy()\n",
    "        elif self.predicted[0] == 0 and self.getposition().size > 0:\n",
    "            self.sell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4dcd10ae-f3e1-46b4-9e55-0ec3d6acb132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "evaluation_metrics = {\n",
    "    \"rf_model\": {\n",
    "        \"Accuracy\": accuracy_rf, \n",
    "        \"Precision\": precision_rf\n",
    "    },\n",
    "    \"xgb_model\": {\n",
    "        \"Accuracy\": accuracy_xgb, \n",
    "        \"Precision\": precision_xgb\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [rf_model, xgb_model]\n",
    "model_names = ['rf_model', 'xgb_model']\n",
    "top_tickers = {}  # Dictionary to store top two tickers for each model\n",
    "ticker_data = pd.read_csv(\"tickers.csv\")  # Ensure this file exists\n",
    "list1 = list(ticker_data[\"Ticker\"])\n",
    "\n",
    "for model, model_name in zip(models, model_names):\n",
    "    final_values = {}\n",
    "    stock_predictions = {}\n",
    "    for ticker in list1:\n",
    "        processed_data = get_stock_predictions(ticker, model)\n",
    "        if processed_data is not None:\n",
    "            stock_predictions[ticker] = processed_data\n",
    "\n",
    "    # Running backtest and storing final portfolio values\n",
    "    for ticker, data in stock_predictions.items():\n",
    "        cerebro = bt.Cerebro()\n",
    "        cerebro.addstrategy(RFStrategy)\n",
    "        data_feed = PredictionsData(dataname=data)\n",
    "        cerebro.adddata(data_feed)\n",
    "        cerebro.broker.set_cash(10000)\n",
    "        cerebro.broker.setcommission(commission=0.001)\n",
    "        cerebro.run()\n",
    "        final_val = cerebro.broker.getvalue()\n",
    "        final_values[ticker] = final_val\n",
    "\n",
    "    # Identify top two tickers\n",
    "    top_two = sorted(final_values, key=final_values.get, reverse=True)[:2]\n",
    "    top_tickers[model_name] = top_two\n",
    "\n",
    "combined_output_df = pd.DataFrame({\n",
    "    \"Model\": ['Random Forest', 'XGBoost'],\n",
    "    \"Top_Ticker_1\": [top_tickers[model][0] for model in model_names],\n",
    "    \"Top_Ticker_2\": [top_tickers[model][1] for model in model_names],\n",
    "    \"Accuracy_1\": [evaluation_metrics[model][\"Accuracy\"] for model in model_names],\n",
    "    \"Precision_1\": [evaluation_metrics[model][\"Precision\"] for model in model_names],\n",
    "    \"Accuracy_2\": [evaluation_metrics[model][\"Accuracy\"] for model in model_names],\n",
    "    \"Precision_2\": [evaluation_metrics[model][\"Precision\"] for model in model_names]\n",
    "})\n",
    "\n",
    "# # Export to CSV\n",
    "combined_output_df.to_csv('small_universe_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b0ec8c2-8dea-4e71-8ea0-48ceb420991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report_for_ticker(ticker, model, strategy_class, stock_predictions):\n",
    "    cerebro = bt.Cerebro()\n",
    "    cerebro.addstrategy(strategy_class)\n",
    "    data_feed = PredictionsData(dataname=stock_predictions[ticker])\n",
    "    cerebro.adddata(data_feed)\n",
    "    cerebro.broker.set_cash(10000)\n",
    "    cerebro.broker.setcommission(commission=0.001)\n",
    "    cerebro.addanalyzer(bt.analyzers.TimeReturn, _name='time_return')\n",
    "    strat = cerebro.run()\n",
    "    daily_returns = strat[0].analyzers.time_return.get_analysis()\n",
    "    returns_series = pd.Series(daily_returns)\n",
    "    returns_series.index = pd.to_datetime(returns_series.index)\n",
    "\n",
    "    # Generate and save reports\n",
    "    qs.reports.html(returns_series, output=f'quantstats_{ticker}_{model}.html')\n",
    "\n",
    "# Generate reports for top tickers of each model\n",
    "for model_name, tickers in top_tickers.items():\n",
    "    for ticker in tickers:\n",
    "        generate_report_for_ticker(ticker, model_name, RFStrategy, stock_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f364b3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_stock_data(data):\n",
    "    if data.empty:\n",
    "        return None, None\n",
    "\n",
    "    stock = Stock(data)\n",
    "    stock.factor()  # Feature creation and preprocessing\n",
    "    stock.standardize()  # Standardizing the data\n",
    "\n",
    "    # Check if data is still non-empty after standardization\n",
    "    if stock.data.size == 0:  # Use .size for NumPy arrays\n",
    "        return None, None\n",
    "\n",
    "    stock.normalize()  # Normalizing the data\n",
    "\n",
    "    X = pd.DataFrame(stock.data)\n",
    "    y = pd.Series(stock.label)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a29e0dea-e8d2-4266-8a45-7059a0183f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(ticker):\n",
    "    try:\n",
    "        data = yf.download(ticker,start=\"2000-01-01\", end=\"2021-11-12\")\n",
    "        if data.empty:\n",
    "            return None\n",
    "        else:\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70364c99-7032-484f-9bd2-931d27fcdb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['PIH']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['FCCY']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['JOBS']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['AVHI']: Exception(\"%ticker%: Data doesn't exist for startDate = 946702800, endDate = 1636693200\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['ABEOW']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['ABIL']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 32)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/48/lm_7bjyx0zjd91lh7j3sylzm0000gn/T/ipykernel_83114/2897723806.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_stock_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/48/lm_7bjyx0zjd91lh7j3sylzm0000gn/T/ipykernel_83114/2538571709.py\u001b[0m in \u001b[0;36mprocess_stock_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Feature creation and preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mstock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Standardizing the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Check if data is still non-empty after standardization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/48/lm_7bjyx0zjd91lh7j3sylzm0000gn/T/ipykernel_83114/3274884737.py\u001b[0m in \u001b[0;36mstandardize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# Now standardize the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m#normalize data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \"\"\"\n\u001b[1;32m    840\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    798\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 32)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "ticker_df = pd.read_csv(\"tickers_nasd.csv\")  \n",
    "tickers = list(ticker_df[\"Symbol\"])\n",
    "results_rf = {}\n",
    "results_xgb = {}\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "accuracy_results_rf = {}\n",
    "accuracy_results_xgb = {}\n",
    "\n",
    "# Initialize a dictionary to store combined accuracies\n",
    "combined_accuracy_results = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    fetched_data = fetch_data(ticker)\n",
    "    if fetched_data is None or fetched_data.empty:\n",
    "        continue\n",
    "\n",
    "    X, y = process_stock_data(fetched_data)\n",
    "    if X is None or y is None or X.empty or len(y) < 2:\n",
    "        continue\n",
    "\n",
    "    # Ensure there's enough data to split\n",
    "    if len(X) > 1:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "        # Random Forest Model\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        accuracy_rf = accuracy_score(y_test, rf_model.predict(X_test))\n",
    "        accuracy_results_rf[ticker] = accuracy_rf  # Store RF accuracy\n",
    "\n",
    "        # XGBoost Model\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        accuracy_xgb = accuracy_score(y_test, xgb_model.predict(X_test))\n",
    "        accuracy_results_xgb[ticker] = accuracy_xgb  # Store XGB accuracy\n",
    "\n",
    "        # Combine accuracies (here, taking the average)\n",
    "        combined_accuracy = (accuracy_rf + accuracy_xgb) / 2\n",
    "        combined_accuracy_results[ticker] = combined_accuracy\n",
    "\n",
    "\n",
    "\n",
    "# Sort and select top 10 tickers based on combined model accuracies\n",
    "top_10_tickers_combined = sorted(combined_accuracy_results, key=combined_accuracy_results.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d97ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and select top 10 tickers based on combined model accuracies\n",
    "top_10_tickers_combined = sorted(combined_accuracy_results, key=combined_accuracy_results.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd65e8f-7324-4ab0-be2d-04206657c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to display the tickers along with accuracies from both models\n",
    "top_10_df = pd.DataFrame({\n",
    "    'Ticker': top_10_tickers_combined,\n",
    "    'Combined_Accuracy': [combined_accuracy_results.get(ticker, None) for ticker in top_10_tickers_combined],\n",
    "    'RF_Accuracy': [accuracy_results_rf.get(ticker, None) for ticker in top_10_tickers_combined],\n",
    "    'XGB_Accuracy': [accuracy_results_xgb.get(ticker, None) for ticker in top_10_tickers_combined]\n",
    "})\n",
    "\n",
    "\n",
    "# Convert the DataFrame to a CSV string and write to a file\n",
    "csv_data = top_10_df.to_csv(index=False)\n",
    "with open('top_10_combined_accuracy.csv', 'w') as file:\n",
    "    file.write(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83707e8c-1ff6-4275-83d8-f98a1d225b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_predictions = {}\n",
    "for ticker in top_10_tickers_combined:\n",
    "    predictions = get_stock_predictions(ticker, rf_model)  # Replace 'your_model' with the actual model\n",
    "    if predictions is not None:\n",
    "        stock_predictions[ticker] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d0f49-7bbc-42b5-b357-0461369e2e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in top_10_tickers_combined:\n",
    "    cerebro = bt.Cerebro()\n",
    "    cerebro.addstrategy(RFStrategy)\n",
    "    data_feed = PredictionsData(dataname=stock_predictions[ticker])\n",
    "    cerebro.adddata(data_feed)\n",
    "    cerebro.broker.set_cash(10000)\n",
    "    cerebro.broker.setcommission(commission=0.001)\n",
    "    cerebro.addanalyzer(bt.analyzers.TimeReturn, _name='time_return')\n",
    "    strat = cerebro.run()\n",
    "    daily_returns = strat[0].analyzers.time_return.get_analysis()\n",
    "    returns_series = pd.Series(daily_returns)\n",
    "    returns_series.index = pd.to_datetime(returns_series.index)\n",
    "\n",
    "    # Generate and save reports\n",
    "    qs.reports.html(returns_series, output=f'quantstats_{ticker}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "12e73f46ad420e8aad983801546adb4157562958827884cec3d9c092c4bd6f79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
