{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e74e7060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (0.2.33)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from yfinance) (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from yfinance) (1.21.5)\n",
      "Requirement already satisfied: requests>=2.31 in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from yfinance) (2.31.0)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from yfinance) (4.9.1)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2022.5 in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from yfinance) (2023.3.post1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from yfinance) (2.3.10)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from yfinance) (3.17.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from yfinance) (4.11.1)\n",
      "Requirement already satisfied: html5lib>=1.1 in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.3.1)\n",
      "Requirement already satisfied: six>=1.9 in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.31->yfinance) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.31->yfinance) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.31->yfinance) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pranuprakash/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.31->yfinance) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1583d3c4-102f-44ba-ae03-fd7dc502e220",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# import yfinance as yf\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, MinMaxScaler\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import xgboost as xgb\n",
    "import backtrader as bt\n",
    "import quantstats as qs\n",
    "import pyfolio as pf\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from fredapi import Fred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2759376f-7d9b-4b9d-9875-b9dfa76e6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgs2_data = pd.read_csv('DGS2.csv')\n",
    "\n",
    "# Convert the 'DATE' column to datetime and the 'DGS2' column to numeric\n",
    "dgs2_data['DATE'] = pd.to_datetime(dgs2_data['DATE'])\n",
    "dgs2_data['DGS2'] = pd.to_numeric(dgs2_data['DGS2'], errors='coerce')\n",
    "\n",
    "# Calculate the daily percentage change of DGS2 values\n",
    "dgs2_data['DGS2_pct_change'] = dgs2_data['DGS2'].pct_change()\n",
    "\n",
    "# Filter the data for the specified date range (2000-01-01 to 2021-11-12)\n",
    "dgs2_filtered = dgs2_data[(dgs2_data['DATE'] >= '2000-01-01') & (dgs2_data['DATE'] <= '2023-12-20')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719e4759-03a1-43f7-a4ee-effafb066867",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stock:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.label = []\n",
    "    \n",
    "    #preprocess\n",
    "    #feature creation\n",
    "    def factor(self):\n",
    "        del self.data[\"Close\"]\n",
    "        self.data = self.data.fillna(method = \"bfill\")\n",
    "        \n",
    "        #return\n",
    "        #>0,+1;<=0,-1\n",
    "        self.data['label']=self.data.rolling(2).apply(lambda x:x.iloc[1]>x.iloc[0])['Adj Close']\n",
    "        self.data['label']=self.data.label.shift(-1)\n",
    "        \n",
    "        self.data['volume'] = self.data['Volume']\n",
    "        self.data['volume_change'] = self.data['Volume'].diff()\n",
    "\n",
    "        # Volume Change (daily percentage change)\n",
    "        self.data['volume_pct_change'] = self.data['Volume'].pct_change()\n",
    "\n",
    "        # Volume Moving Averages\n",
    "        self.data['vol_ma_5'] = self.data['Volume'].rolling(window=5).mean()\n",
    "        self.data['vol_ma_10'] = self.data['Volume'].rolling(window=10).mean()\n",
    "        self.data['vol_ma_20'] = self.data['Volume'].rolling(window=20).mean()  \n",
    "        self.data['vol_ma_50'] = self.data['Volume'].rolling(window=50).mean()  \n",
    "        self.data['vol_ma_200'] = self.data['Volume'].rolling(window=200).mean()  \n",
    "\n",
    "        #moving average\n",
    "        self.data[\"ma_5\"] = self.data[\"Adj Close\"].rolling(window = 5).mean()\n",
    "        self.data[\"ma_10\"] = self.data[\"Adj Close\"].rolling(window = 10).mean()\n",
    "        self.data[\"ma_20\"] = self.data[\"Adj Close\"].rolling(window = 20).mean()\n",
    "        self.data[\"ma_50\"] = self.data[\"Adj Close\"].rolling(window = 50).mean()\n",
    "        self.data[\"ma_200\"] = self.data[\"Adj Close\"].rolling(window = 200).mean()\n",
    "\n",
    "        self.add_volume_oscillator()\n",
    "        self.add_relative_volume()\n",
    "        self.add_volume_relative_to_ma()\n",
    "        self.add_volume_spikes()\n",
    "        self.add_price_volume_trend()\n",
    "        #self.add_vix_feature()\n",
    "        self.add_spy_vix_ratio_feature()\n",
    "        self.add_spy_iwm_ratio_feature()\n",
    "        self.add_rsi_feature()\n",
    "        self.add_bollinger_bands_feature()\n",
    "        self.add_cci_feature()\n",
    "        #self.add_stochastic_oscillator()\n",
    "        #self.add_spy_qqq_ratio_feature()\n",
    "        #self.add_spy_dia_ratio_feature()\n",
    "        #self.add_dxy_feature()\n",
    "        #macd\n",
    "        self.add_10yr_bond_yield_change_feature()\n",
    "        #self.add_2yr_treasury_yield_change_feature(dgs2_filtered)  # Correctly pass dgs2_filtered as an argument\n",
    "        self.add_yield_spread_feature(dgs2_filtered)\n",
    "        self.add_atr_feature()\n",
    "        #self.add_obv_feature()\n",
    "        ema_short = self.data['Adj Close'].ewm(span=12, adjust=False, min_periods=12).mean()\n",
    "        ema_long = self.data['Adj Close'].ewm(span=26, adjust=False, min_periods=26).mean()\n",
    "        macd = ema_short - ema_long\n",
    "        macd_s = macd.ewm(span=9, adjust=False, min_periods=9).mean()\n",
    "        macd_h = macd - macd_s\n",
    "        self.data['macd'] = macd\n",
    "        self.data['macd_h'] = macd_h\n",
    "        self.data['macd_s'] = macd_s\n",
    "        \n",
    "        self.data = self.data.dropna(how = \"any\")\n",
    "        self.label = list(self.data[\"label\"])\n",
    "        del self.data[\"label\"]\n",
    "        del self.data[\"Adj Close\"]\n",
    "               \n",
    "    def add_volume_oscillator(self):\n",
    "            \"\"\"Add Volume Oscillator feature.\"\"\"\n",
    "            short_term = 5\n",
    "            long_term = 10\n",
    "            self.data['vol_ma_short'] = self.data['Volume'].rolling(window=short_term).mean()\n",
    "            self.data['vol_ma_long'] = self.data['Volume'].rolling(window=long_term).mean()\n",
    "            self.data['volume_oscillator'] = self.data['vol_ma_short'] - self.data['vol_ma_long']\n",
    "\n",
    "    def add_relative_volume(self, comparison_period=20):\n",
    "        \"\"\"Add Relative Volume feature.\"\"\"\n",
    "        # Calculate the average volume over the specified comparison period\n",
    "        self.data['avg_volume'] = self.data['Volume'].rolling(window=comparison_period).mean()\n",
    "\n",
    "        # Calculate Relative Volume\n",
    "        self.data['relative_volume'] = self.data['Volume'] / self.data['avg_volume']\n",
    "\n",
    "    def add_volume_relative_to_ma(self, period=50):\n",
    "        \"\"\"Add Volume Relative to Moving Average.\"\"\"\n",
    "        self.data['vol_relative_to_ma'] = self.data['Volume'] / self.data['Volume'].rolling(window=period).mean()\n",
    "\n",
    "    def add_volume_spikes(self, threshold=2):\n",
    "        \"\"\"Add Volume Spikes.\"\"\"\n",
    "        self.data['vol_spike'] = self.data['Volume'] > self.data['Volume'].rolling(window=50).mean() * threshold\n",
    "\n",
    "    def add_price_volume_trend(self):\n",
    "        \"\"\"Add Price-Volume Trend.\"\"\"\n",
    "        self.data['pvt'] = (self.data['Volume'] * self.data['Adj Close'].diff()).cumsum()\n",
    "\n",
    "    def download_vix(self):\n",
    "        \"\"\"Download VIX data for the same date range as the stock data.\"\"\"\n",
    "        start_date = self.data.index.min().strftime('%Y-%m-%d')\n",
    "        end_date = self.data.index.max().strftime('%Y-%m-%d')\n",
    "        self.vix_data = yf.download(\"^VIX\", start=\"2000-01-01\", end=\"2023-12-20\")['Close']\n",
    "\n",
    "    def add_vix_feature(self):\n",
    "        \"\"\"Add VIX as a feature along with its percentage change.\"\"\"\n",
    "        self.download_vix()  # Download VIX data\n",
    "\n",
    "        # Merge raw VIX data into the stock data\n",
    "        self.data['vix'] = self.vix_data.reindex(self.data.index, method='bfill')\n",
    "\n",
    "        # Calculate the percentage change in VIX\n",
    "        self.data['vix_pct_change'] = self.data['vix'].pct_change()\n",
    "\n",
    "        # Handle any missing values\n",
    "        self.data['vix_pct_change'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "\n",
    "    def download_spy(self):\n",
    "        \"\"\"Download SPY data for the same date range as the stock data.\"\"\"\n",
    "        start_date = self.data.index.min().strftime('%Y-%m-%d')\n",
    "        end_date = self.data.index.max().strftime('%Y-%m-%d')\n",
    "        self.spy_data = yf.download(\"SPY\", start=\"2000-01-01\", end=\"2023-12-20\")['Adj Close']\n",
    "\n",
    "    def add_spy_vix_ratio_feature(self):\n",
    "        \"\"\"Add feature of SPY price change / VIX price change.\"\"\"\n",
    "        self.download_spy()  # Download SPY data\n",
    "        self.download_vix()  # Download VIX data\n",
    "\n",
    "        # Calculate daily percentage change for SPY and VIX\n",
    "        spy_pct_change = self.spy_data.pct_change()\n",
    "        vix_pct_change = self.vix_data.pct_change()\n",
    "        \n",
    "        # Calculate the ratio of SPY change to VIX change\n",
    "        self.data['SPY_VIX_ratio'] = spy_pct_change/ vix_pct_change\n",
    "        self.data['SPY_VIX_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.data['SPY_VIX_ratio'] = self.data['SPY_VIX_ratio'].fillna(method = \"bfill\")\n",
    "\n",
    "    def download_etf_data(self, ticker, column_name):\n",
    "        \"\"\"Download ETF data for the same date range as the stock data.\"\"\"\n",
    "        start_date = self.data.index.min().strftime('%Y-%m-%d')\n",
    "        end_date = self.data.index.max().strftime('%Y-%m-%d')\n",
    "        etf_data = yf.download(ticker, start=\"2000-01-01\", end=\"2023-12-20\")['Adj Close']\n",
    "        etf_pct_change = etf_data.pct_change()\n",
    "        self.data[column_name] = etf_pct_change\n",
    "\n",
    "    def add_spy_iwm_ratio_feature(self):\n",
    "        \"\"\"Add feature of SPY price change / IWM price change.\"\"\"\n",
    "        self.download_etf_data(\"SPY\", \"SPY_pct_change\")\n",
    "        self.download_etf_data(\"IWM\", \"IWM_pct_change\")\n",
    "\n",
    "        # Calculate the ratio of SPY change to IWM change\n",
    "        self.data['SPY_IWM_ratio'] = self.data['SPY_pct_change'] / self.data['IWM_pct_change']\n",
    "        self.data['SPY_IWM_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.data['SPY_IWM_ratio'] = self.data['SPY_VIX_ratio'].fillna(method = \"bfill\")\n",
    "    def add_spy_qqq_ratio_feature(self):\n",
    "        \"\"\"Add feature of SPY price change / QQQ price change.\"\"\"\n",
    "        self.download_etf_data(\"SPY\", \"SPY_pct_change\")\n",
    "        self.download_etf_data(\"QQQ\", \"QQQ_pct_change\")\n",
    "        self.data['SPY_QQQ_ratio'] = self.data['SPY_pct_change'] / self.data['QQQ_pct_change']\n",
    "        self.data['SPY_QQQ_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.data['SPY_QQQ_ratio'] = self.data['SPY_VIX_ratio'].fillna(method = \"bfill\")\n",
    "    def add_spy_dia_ratio_feature(self):\n",
    "        \"\"\"Add feature of SPY price change / DIA price change.\"\"\"\n",
    "        self.download_etf_data(\"SPY\", \"SPY_pct_change\")\n",
    "        self.download_etf_data(\"DIA\", \"DIA_pct_change\")\n",
    "        self.data['SPY_DIA_ratio'] = self.data['SPY_pct_change'] / self.data['DIA_pct_change']\n",
    "        self.data['SPY_DIA_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.data['SPY_DIA_ratio'] = self.data['SPY_VIX_ratio'].fillna(method = \"bfill\")\n",
    "\n",
    "    def add_dxy_feature(self):\n",
    "        \"\"\"Add feature of DXY (US Dollar Index) daily percentage change.\"\"\"\n",
    "    # Download DXY data for the same date range as the stock data\n",
    "\n",
    "        dxy_data = yf.download(\"DX-Y.NYB\", start=\"2000-01-01\", end=\"2023-12-20\")['Adj Close']\n",
    "        \n",
    "        # Calculate daily percentage change for DXY\n",
    "        dxy_pct_change = dxy_data.pct_change()\n",
    "        \n",
    "        # Add the DXY daily percentage change to the stock data DataFrame\n",
    "        self.data['DXY_pct_change'] = dxy_pct_change\n",
    "        self.data['DXY_pct_change'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.data['DXY_pct_change'] = self.data['DXY_pct_change'].fillna(method=\"bfill\")\n",
    "    def download_bond_yield_data(self, ticker, column_name):\n",
    "        \"\"\"Download bond yield data for the same date range as the stock data.\"\"\"\n",
    "\n",
    "        bond_data = yf.download(ticker, start=\"2000-01-01\", end=\"2023-12-20\")['Adj Close']\n",
    "        bond_pct_change = bond_data.pct_change()\n",
    "        self.data[column_name] = bond_pct_change\n",
    "\n",
    "    def add_2yr_treasury_yield_change_feature(self, dgs2_data):\n",
    "        \"\"\"\n",
    "        Add the 2-year Treasury yield percentage change as a feature to the stock data.\n",
    "        \n",
    "        Args:\n",
    "        dgs2_data (DataFrame): DataFrame containing the DGS2 data with 'DATE' and 'DGS2_pct_change' columns.\n",
    "        \"\"\"\n",
    "        # Check if 'DATE' column exists in dgs2_data\n",
    "        if 'DATE' in dgs2_data.columns:\n",
    "            # If 'DATE' column exists, ensure it's in datetime format and set it as the index\n",
    "            dgs2_data['DATE'] = pd.to_datetime(dgs2_data['DATE'])\n",
    "            dgs2_data.set_index('DATE', inplace=True)\n",
    "        \n",
    "        # Merge the DGS2_pct_change into the stock data\n",
    "        self.data = self.data.join(dgs2_data['DGS2_pct_change'], how='left')\n",
    "\n",
    "        # Handle any infinite values and fill missing values\n",
    "        self.data['DGS2_pct_change'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.data['DGS2_pct_change'] = self.data['DGS2_pct_change'].fillna(method=\"bfill\")\n",
    "    \n",
    "    def add_10yr_bond_yield_change_feature(self):\n",
    "        \"\"\"Add change of 10-year bond yield as a feature.\"\"\"\n",
    "        self.download_bond_yield_data(\"^TNX\", \"10yr_bond_yield_change\")\n",
    "\n",
    "    def add_yield_spread_feature(self, dgs2_data):\n",
    "        \"\"\"\n",
    "        Add the feature representing the difference between 2-year and 10-year Treasury bond yields.\n",
    "        \n",
    "        Args:\n",
    "        dgs2_data (DataFrame): DataFrame containing the local DGS2 data.\n",
    "        \"\"\"\n",
    "        # Download 10-year Treasury yield data\n",
    "        dgs10_data = yf.download(\"^TNX\", start=\"2000-01-01\", end=\"2023-12-20\")['Adj Close']\n",
    "\n",
    "        # Ensure DGS2 data is in the correct format\n",
    "        if 'DATE' in dgs2_data.columns:\n",
    "            dgs2_data.set_index('DATE', inplace=True)\n",
    "        dgs2_data.index = pd.to_datetime(dgs2_data.index)\n",
    "\n",
    "        # Align the DGS10 data with DGS2 data dates\n",
    "        dgs10_aligned = dgs10_data.reindex(dgs2_data.index, method='bfill')\n",
    "\n",
    "        # Calculate the yield spread\n",
    "        yield_spread = dgs10_aligned - dgs2_data['DGS2']\n",
    "\n",
    "        # Add the yield spread to the stock data\n",
    "        self.data['yield_spread'] = yield_spread\n",
    "\n",
    "        # Handle missing values\n",
    "        self.data['yield_spread'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "    def add_rsi_feature(self, window=14):\n",
    "        \"\"\"Add Relative Strength Index (RSI) feature.\"\"\"\n",
    "        delta = self.data['Adj Close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "\n",
    "        rs = gain / loss\n",
    "        self.data['rsi'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    def add_bollinger_bands_feature(self, window=20, num_std=2):\n",
    "        \"\"\"Add Bollinger Bands feature.\"\"\"\n",
    "        rolling_mean = self.data['Adj Close'].rolling(window=window).mean()\n",
    "        rolling_std = self.data['Adj Close'].rolling(window=window).std()\n",
    "\n",
    "        self.data['bollinger_upper'] = rolling_mean + (rolling_std * num_std)\n",
    "        self.data['bollinger_lower'] = rolling_mean - (rolling_std * num_std)\n",
    "\n",
    "    def add_atr_feature(self, window=14):\n",
    "        \"\"\"Add Average True Range (ATR) feature.\"\"\"\n",
    "        high_low = self.data['High'] - self.data['Low']\n",
    "        high_close = np.abs(self.data['High'] - self.data['Adj Close'].shift())\n",
    "        low_close = np.abs(self.data['Low'] - self.data['Adj Close'].shift())\n",
    "\n",
    "        ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
    "        true_range = np.max(ranges, axis=1)\n",
    "        self.data['atr'] = true_range.rolling(window=window).mean()\n",
    "\n",
    "    def add_stochastic_oscillator(self, k_window=14, d_window=3):\n",
    "        \"\"\"Add Stochastic Oscillator feature.\"\"\"\n",
    "        min_low = self.data['Low'].rolling(window=k_window).min()\n",
    "        max_high = self.data['High'].rolling(window=k_window).max()\n",
    "\n",
    "        self.data['%K'] = 100 * ((self.data['Adj Close'] - min_low) / (max_high - min_low))\n",
    "        self.data['%D'] = self.data['%K'].rolling(window=d_window).mean()\n",
    "\n",
    "    def add_cci_feature(self, window=20):\n",
    "        \"\"\"Add Commodity Channel Index (CCI) feature.\"\"\"\n",
    "        tp = (self.data['High'] + self.data['Low'] + self.data['Adj Close']) / 3\n",
    "        cci = (tp - tp.rolling(window=window).mean()) / (0.015 * tp.rolling(window=window).std())\n",
    "        self.data['cci'] = cci\n",
    "\n",
    "    def add_obv_feature(self):\n",
    "        \"\"\"Add On-Balance Volume (OBV) feature.\"\"\"\n",
    "        obv = (np.sign(self.data['Adj Close'].diff()) * self.data['Volume']).fillna(0).cumsum()\n",
    "        self.data['obv'] = obv\n",
    "\n",
    "    #standardize data\n",
    "    def standardize(self):\n",
    "        scaler = StandardScaler()      \n",
    "        self.data = scaler.fit_transform(self.data)\n",
    "    \n",
    "    #normalize data\n",
    "    def normalize(self):\n",
    "        scaler = MinMaxScaler()      \n",
    "        self.data = scaler.fit_transform(self.data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699943e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Assuming Stock class and list1 are defined as before\n",
    "\n",
    "# Lists to store combined features and labels for all stocks\n",
    "all_features_xgb = []\n",
    "all_labels_xgb = []\n",
    "\n",
    "# Download and preprocess data for each ticker in the list\n",
    "yf_data = yf.download(\"SPY\", start=\"2000-01-01\", end=\"2021-11-12\")\n",
    "stock = Stock(yf_data)\n",
    "stock.factor()        # Feature creation and preprocessing\n",
    "stock.standardize()   # Standardizing the data\n",
    "stock.normalize()     # Normalizing the data\n",
    "\n",
    "# Append the features and labels to the respective lists\n",
    "all_features_xgb.append(pd.DataFrame(stock.data))\n",
    "all_labels_xgb.extend(stock.label)\n",
    "\n",
    "X = pd.concat(all_features_xgb, ignore_index=True)\n",
    "y = pd.Series(all_labels_xgb)\n",
    "\n",
    "# Set a fixed random state for reproducibility\n",
    "fixed_random_state = 42\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=fixed_random_state,shuffle=False)\n",
    "\n",
    "# Initialize an XGBoost classifier with adjusted parameters\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    max_depth=6,\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=fixed_random_state\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "pred_y = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculating accuracy and precision\n",
    "accuracy_xgb = accuracy_score(y_test, pred_y)\n",
    "precision_xgb = precision_score(y_test, pred_y)\n",
    "print(\"accuracy_xgb: \", accuracy_xgb)\n",
    "print(\"precision_xgb: \", precision_xgb)\n",
    "cm = confusion_matrix(y_test, pred_y)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195208ab-8cdf-40ff-b98f-0e46ba22d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate, TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "import xgboost as xgb\n",
    "\n",
    "# Assuming the Stock class is defined as before\n",
    "\n",
    "# Download and preprocess data\n",
    "yf_data = yf.download(\"SPY\", start=\"2000-01-01\", end=\"2021-11-12\")\n",
    "stock = Stock(yf_data)\n",
    "stock.factor()        # Feature creation and preprocessing\n",
    "stock.standardize()   # Standardizing the data\n",
    "stock.normalize()     # Normalizing the data\n",
    "\n",
    "# Create feature matrix X and target vector y\n",
    "X = pd.DataFrame(stock.data)\n",
    "y = pd.Series(stock.label)\n",
    "\n",
    "# Initialize an XGBoost classifier with adjusted parameters\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    max_depth=6,\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=fixed_random_state\n",
    ")\n",
    "\n",
    "# Define the number of folds for cross-validation and scoring metrics\n",
    "num_folds = 5\n",
    "scoring_metrics = {'accuracy': make_scorer(accuracy_score), \n",
    "                   'precision': make_scorer(precision_score)}\n",
    "\n",
    "# Use TimeSeriesSplit for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=num_folds)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate(xgb_model, X, y, cv=tscv, scoring=scoring_metrics, return_train_score=True)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Cross-Validation Accuracy Scores: {cv_results['test_accuracy']}\")\n",
    "print(f\"Mean CV Accuracy: {cv_results['test_accuracy'].mean()}\")\n",
    "print(f\"Cross-Validation Precision Scores: {cv_results['test_precision']}\")\n",
    "print(f\"Mean CV Precision: {cv_results['test_precision'].mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a060e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Ensure this file exists in your directory\n",
    "list1 = {\"SPY\"}\n",
    "# Assuming Stock class and list1 are defined as before\n",
    "\n",
    "# Lists to store combined features and labels for all stocks\n",
    "all_features_rf = []\n",
    "all_labels_rf = []\n",
    "\n",
    "for tic in list1:\n",
    "    # Download and preprocess data for each ticker in the list\n",
    "    yf_data = yf.download(tic, start=\"2000-01-01\", end=\"2021-11-12\")\n",
    "    \n",
    "    # Skip if data is empty\n",
    "    if yf_data.empty:\n",
    "        continue\n",
    "\n",
    "    stock = Stock(yf_data)\n",
    "    stock.factor()        # Feature creation and preprocessing\n",
    "    stock.standardize()   # Standardizing the data\n",
    "    stock.normalize() \n",
    "\n",
    "    # Append the features and labels to the respective lists\n",
    "    all_features_rf.append(pd.DataFrame(stock.data))\n",
    "    all_labels_rf.extend(stock.label)\n",
    "\n",
    "# Concatenating all features and labels into single datasets\n",
    "X = pd.concat(all_features_rf)\n",
    "y = pd.Series(all_labels_rf)\n",
    "\n",
    "# Set a fixed random state for reproducibility\n",
    "fixed_random_state = 42\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=fixed_random_state,shuffle=False)\n",
    "\n",
    "# Initialize a Random Forest classifier with adjusted parameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=150,    # Increased number of trees\n",
    "    max_depth=10,        # Maximum depth of each tree\n",
    "    min_samples_split=4, # Minimum number of samples required to split an internal node\n",
    "    min_samples_leaf=2,  # Minimum number of samples required to be at a leaf node\n",
    "    random_state=fixed_random_state  # Random state for reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "pred_y_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Calculating accuracy and precision for the combined dataset\n",
    "accuracy_rf = accuracy_score(y_test, pred_y_rf)\n",
    "precision_rf = precision_score(y_test, pred_y_rf)\n",
    "print(\"accuracy_rf: \", accuracy_rf)\n",
    "print(\"precision_rf: \", precision_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91371e9c-b34f-4545-87f0-07033fd74283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate, TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Ensure this file exists in your directory\n",
    "list1 = {\"SPY\"}\n",
    "# Assuming Stock class and list1 are defined as before\n",
    "\n",
    "# Lists to store combined features and labels for all stocks\n",
    "all_features_rf = []\n",
    "all_labels_rf = []\n",
    "\n",
    "for tic in list1:\n",
    "    # Download and preprocess data for each ticker in the list\n",
    "    yf_data = yf.download(tic, start=\"2000-01-01\", end=\"2021-11-12\")\n",
    "    \n",
    "    # Skip if data is empty\n",
    "    if yf_data.empty:\n",
    "        continue\n",
    "\n",
    "    stock = Stock(yf_data)\n",
    "    stock.factor()        # Feature creation and preprocessing\n",
    "    stock.standardize()   # Standardizing the data\n",
    "    stock.normalize()     # Normalizing the data\n",
    "\n",
    "    # Append the features and labels to the respective lists\n",
    "    all_features_rf.append(pd.DataFrame(stock.data))\n",
    "    all_labels_rf.extend(stock.label)\n",
    "\n",
    "# Concatenating all features and labels into single datasets\n",
    "X = pd.concat(all_features_rf)\n",
    "y = pd.Series(all_labels_rf)\n",
    "\n",
    "# Initialize a Random Forest classifier with adjusted parameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=150,    # Increased number of trees\n",
    "    max_depth=10,        # Maximum depth of each tree\n",
    "    min_samples_split=4, # Minimum number of samples required to split an internal node\n",
    "    min_samples_leaf=2,  # Minimum number of samples required to be at a leaf node\n",
    "    random_state=42      # Random state for reproducibility\n",
    ")\n",
    "\n",
    "# Define the number of folds for cross-validation and scoring metrics\n",
    "num_folds = 5\n",
    "scoring_metrics = {'accuracy': make_scorer(accuracy_score), \n",
    "                   'precision': make_scorer(precision_score)}\n",
    "\n",
    "# Use TimeSeriesSplit for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=num_folds)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate(rf_model, X, y, cv=tscv, scoring=scoring_metrics, return_train_score=True)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Cross-Validation Accuracy Scores: {cv_results['test_accuracy']}\")\n",
    "print(f\"Mean CV Accuracy: {cv_results['test_accuracy'].mean()}\")\n",
    "print(f\"Cross-Validation Precision Scores: {cv_results['test_precision']}\")\n",
    "print(f\"Mean CV Precision: {cv_results['test_precision'].mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d9fa0-9845-4197-bc65-ee8e6884448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = pd.read_csv(\"tickers.csv\")  # Ensure this file exists in your directory\n",
    "list1 = list(ticker[\"Ticker\"])\n",
    "# Custom data loading class for backtrader\n",
    "class PredictionsData(bt.feeds.PandasData):\n",
    "    lines = ('predictions',)\n",
    "    params = (('predictions', -1),)\n",
    "\n",
    "def get_stock_predictions(ticker, model):\n",
    "    yf_data = yf.download(ticker, start=\"2000-01-01\", end=\"2021-11-12\")\n",
    "    if yf_data.empty:\n",
    "        return None\n",
    "\n",
    "    # Preprocess the data\n",
    "    stock = Stock(yf_data.copy())\n",
    "    stock.factor()\n",
    "    stock.standardize()\n",
    "    stock.normalize()\n",
    "\n",
    "    # Splitting the processed data\n",
    "    X = pd.DataFrame(stock.data)\n",
    "    _, X_test = train_test_split(X, test_size=0.4, random_state=42)\n",
    "\n",
    "    # Generate predictions for the test set\n",
    "    test_predictions = model.predict(X_test)\n",
    "\n",
    "    # Store predictions in a DataFrame with the same index as the original data\n",
    "    predictions_series = pd.Series(index=yf_data.index)\n",
    "    predictions_series[X_test.index] = test_predictions\n",
    "    yf_data['predictions'] = predictions_series\n",
    "\n",
    "    return yf_data\n",
    "\n",
    "# Backtrader strategy class\n",
    "class RFStrategy(bt.Strategy):\n",
    "    def __init__(self):\n",
    "        self.predicted = self.datas[0].predictions\n",
    "\n",
    "    def next(self):\n",
    "        if not self.position:\n",
    "            if self.predicted[0] == 1 and self.broker.get_cash() > 100:\n",
    "                self.buy()\n",
    "        elif self.predicted[0] == 0 and self.getposition().size > 0:\n",
    "            self.sell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd10ae-f3e1-46b4-9e55-0ec3d6acb132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "evaluation_metrics = {\n",
    "    \"rf_model\": {\n",
    "        \"Accuracy\": accuracy_rf, \n",
    "        \"Precision\": precision_rf\n",
    "    },\n",
    "    \"xgb_model\": {\n",
    "        \"Accuracy\": accuracy, \n",
    "        \"Precision\": precision\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [rf_model, xgb_model]\n",
    "model_names = ['rf_model', 'xgb_model']\n",
    "top_tickers = {}  # Dictionary to store top two tickers for each model\n",
    "ticker_data = pd.read_csv(\"tickers.csv\")  # Ensure this file exists\n",
    "list1 = list(ticker_data[\"Ticker\"])\n",
    "\n",
    "for model, model_name in zip(models, model_names):\n",
    "    final_values = {}\n",
    "    stock_predictions = {}\n",
    "    for ticker in list1:\n",
    "        processed_data = get_stock_predictions(ticker, model)\n",
    "        if processed_data is not None:\n",
    "            stock_predictions[ticker] = processed_data\n",
    "\n",
    "    # Running backtest and storing final portfolio values\n",
    "    for ticker, data in stock_predictions.items():\n",
    "        cerebro = bt.Cerebro()\n",
    "        cerebro.addstrategy(RFStrategy)\n",
    "        data_feed = PredictionsData(dataname=data)\n",
    "        cerebro.adddata(data_feed)\n",
    "        cerebro.broker.set_cash(10000)\n",
    "        cerebro.broker.setcommission(commission=0.001)\n",
    "        cerebro.run()\n",
    "        final_val = cerebro.broker.getvalue()\n",
    "        final_values[ticker] = final_val\n",
    "\n",
    "    # Identify top two tickers\n",
    "    top_two = sorted(final_values, key=final_values.get, reverse=True)[:2]\n",
    "    top_tickers[model_name] = top_two\n",
    "\n",
    "combined_output_df = pd.DataFrame({\n",
    "    \"Model\": ['Random Forest', 'XGBoost'],\n",
    "    \"Top_Ticker_1\": [top_tickers[model][0] for model in model_names],\n",
    "    \"Top_Ticker_2\": [top_tickers[model][1] for model in model_names],\n",
    "    \"Accuracy_1\": [evaluation_metrics[model][\"Accuracy\"] for model in model_names],\n",
    "    \"Precision_1\": [evaluation_metrics[model][\"Precision\"] for model in model_names],\n",
    "    \"Accuracy_2\": [evaluation_metrics[model][\"Accuracy\"] for model in model_names],\n",
    "    \"Precision_2\": [evaluation_metrics[model][\"Precision\"] for model in model_names]\n",
    "})\n",
    "\n",
    "# # Export to CSV\n",
    "combined_output_df.to_csv('small_universe_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ec8c2-8dea-4e71-8ea0-48ceb420991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report_for_ticker(ticker, model, strategy_class, stock_predictions):\n",
    "    cerebro = bt.Cerebro()\n",
    "    cerebro.addstrategy(strategy_class)\n",
    "    data_feed = PredictionsData(dataname=stock_predictions[ticker])\n",
    "    cerebro.adddata(data_feed)\n",
    "    cerebro.broker.set_cash(10000)\n",
    "    cerebro.broker.setcommission(commission=0.001)\n",
    "    cerebro.addanalyzer(bt.analyzers.TimeReturn, _name='time_return')\n",
    "    strat = cerebro.run()\n",
    "    daily_returns = strat[0].analyzers.time_return.get_analysis()\n",
    "    returns_series = pd.Series(daily_returns)\n",
    "    returns_series.index = pd.to_datetime(returns_series.index)\n",
    "\n",
    "    # Generate and save reports\n",
    "    qs.reports.html(returns_series, output=f'quantstats_{ticker}_{model}.html')\n",
    "\n",
    "# Generate reports for top tickers of each model\n",
    "for model_name, tickers in top_tickers.items():\n",
    "    for ticker in tickers:\n",
    "        generate_report_for_ticker(ticker, model_name, RFStrategy, stock_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f364b3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_stock_data(data):\n",
    "    if data.empty:\n",
    "        return None, None\n",
    "\n",
    "    stock = Stock(data)\n",
    "    stock.factor()  # Feature creation and preprocessing\n",
    "    stock.standardize()  # Standardizing the data\n",
    "\n",
    "    # Check if data is still non-empty after standardization\n",
    "    if stock.data.size == 0:  # Use .size for NumPy arrays\n",
    "        return None, None\n",
    "\n",
    "    stock.normalize()  # Normalizing the data\n",
    "\n",
    "    X = pd.DataFrame(stock.data)\n",
    "    y = pd.Series(stock.label)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e0dea-e8d2-4266-8a45-7059a0183f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(ticker):\n",
    "    try:\n",
    "        data = yf.download(ticker,start=\"2000-01-01\", end=\"2021-11-12\")\n",
    "        if data.empty:\n",
    "            return None\n",
    "        else:\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70364c99-7032-484f-9bd2-931d27fcdb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_df = pd.read_csv(\"tickers_nasd.csv\")  \n",
    "tickers = list(ticker_df[\"Symbol\"])\n",
    "results_rf = {}\n",
    "results_xgb = {}\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "accuracy_results_rf = {}\n",
    "accuracy_results_xgb = {}\n",
    "\n",
    "# Initialize a dictionary to store combined accuracies\n",
    "combined_accuracy_results = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    fetched_data = fetch_data(ticker)\n",
    "    if fetched_data is None or fetched_data.empty:\n",
    "        continue\n",
    "\n",
    "    X, y = process_stock_data(fetched_data)\n",
    "    if X is None or y is None or X.empty or len(y) < 2:\n",
    "        continue\n",
    "\n",
    "    # Ensure there's enough data to split\n",
    "    if len(X) > 1:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "        # Random Forest Model\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        accuracy_rf = accuracy_score(y_test, rf_model.predict(X_test))\n",
    "        accuracy_results_rf[ticker] = accuracy_rf  # Store RF accuracy\n",
    "\n",
    "        # XGBoost Model\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        accuracy_xgb = accuracy_score(y_test, xgb_model.predict(X_test))\n",
    "        accuracy_results_xgb[ticker] = accuracy_xgb  # Store XGB accuracy\n",
    "\n",
    "        # Combine accuracies (here, taking the average)\n",
    "        combined_accuracy = (accuracy_rf + accuracy_xgb) / 2\n",
    "        combined_accuracy_results[ticker] = combined_accuracy\n",
    "\n",
    "\n",
    "\n",
    "# Sort and select top 10 tickers based on combined model accuracies\n",
    "top_10_tickers_combined = sorted(combined_accuracy_results, key=combined_accuracy_results.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d97ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and select top 10 tickers based on combined model accuracies\n",
    "top_10_tickers_combined = sorted(combined_accuracy_results, key=combined_accuracy_results.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd65e8f-7324-4ab0-be2d-04206657c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to display the tickers along with accuracies from both models\n",
    "top_10_df = pd.DataFrame({\n",
    "    'Ticker': top_10_tickers_combined,\n",
    "    'Combined_Accuracy': [combined_accuracy_results.get(ticker, None) for ticker in top_10_tickers_combined],\n",
    "    'RF_Accuracy': [accuracy_results_rf.get(ticker, None) for ticker in top_10_tickers_combined],\n",
    "    'XGB_Accuracy': [accuracy_results_xgb.get(ticker, None) for ticker in top_10_tickers_combined]\n",
    "})\n",
    "\n",
    "\n",
    "# Convert the DataFrame to a CSV string and write to a file\n",
    "csv_data = top_10_df.to_csv(index=False)\n",
    "with open('top_10_combined_accuracy.csv', 'w') as file:\n",
    "    file.write(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83707e8c-1ff6-4275-83d8-f98a1d225b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_predictions = {}\n",
    "for ticker in top_10_tickers_combined:\n",
    "    predictions = get_stock_predictions(ticker, rf_model)  # Replace 'your_model' with the actual model\n",
    "    if predictions is not None:\n",
    "        stock_predictions[ticker] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d0f49-7bbc-42b5-b357-0461369e2e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in top_10_tickers_combined:\n",
    "    cerebro = bt.Cerebro()\n",
    "    cerebro.addstrategy(RFStrategy)\n",
    "    data_feed = PredictionsData(dataname=stock_predictions[ticker])\n",
    "    cerebro.adddata(data_feed)\n",
    "    cerebro.broker.set_cash(10000)\n",
    "    cerebro.broker.setcommission(commission=0.001)\n",
    "    cerebro.addanalyzer(bt.analyzers.TimeReturn, _name='time_return')\n",
    "    strat = cerebro.run()\n",
    "    daily_returns = strat[0].analyzers.time_return.get_analysis()\n",
    "    returns_series = pd.Series(daily_returns)\n",
    "    returns_series.index = pd.to_datetime(returns_series.index)\n",
    "\n",
    "    # Generate and save reports\n",
    "    qs.reports.html(returns_series, output=f'quantstats_{ticker}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
